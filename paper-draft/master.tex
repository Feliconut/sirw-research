	%%%%%%%%%%%%%
% % Lines starting with % are comments, which are ignored.
% % This is a handy way of indicating the date and version of
% % your document, to wit:
% %
% % first draft, 2023_08_03
% % Modified  2023_09_020
% % Lastest Edition, 2023_09_20
% % Description of changes:
% %  Major changes are in subsection 3.1, 3.2: 
% % construction of BLPs
% % 
% % Changes to be made: 
% % (1) subsction 3.2: motivations after constructions  
% % (2) Motivations for 2.2 for 2.2; 
% % (3) rework of descriptions after proposition 2.2, especially a focus on lambda_{x,m} instead of time k: 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Title and author(s)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{Scaling Limit of AF-SIRW for $p\leq\frac{1}{2}$.}
\author{ 
	Xiaoyu Liu
	\and
	Zhe Wang}

\date{Sep 20th, 2023}


% This is the definition of the type of document
\documentclass[twoside,12pt,a4paper]{article}
%\documentclass{article}

\usepackage[english]{babel}
\usepackage[margin=1.0 in]{geometry}
\usepackage[latin1]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fourier}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage[inline]{enumitem}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{csquotes}
\usepackage[backend=biber,style=draft,sorting=none]{biblatex}

\addbibresource{zotero.bib}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{scolium}{Scolium} [section]  
\newtheorem{definition}{Definition}[section]
\numberwithin{equation}{section}

%% This defines the "proo" environment, which is the same as proof, but
\newenvironment{proof}[1][Proof]{{\sc #1}:}{~\hfill $\square$}
\newenvironment{AMS}{}{}
\newenvironment{keywords}{}{}

%% Local macros
\newcommand{\abs}[1]{\left\vert #1 \right\vert}
\newcommand\TBD{\textcolor{red}{TBD.}}
\newcommand{\edt}[1]{\textcolor{red}{#1}} %edit time 09/20/2023
\newcommand{\comment}[1]{\textcolor{blue}{#1}}




\begin{document}
	\maketitle

	\setcounter{page}{1} 
	
	\begin{abstract}
		This document is an outline of the article for the Scaling limit of SIRW. We complete the functional CLT in \cite{KMP22} for the asymptotically free self-interacting random walk (AF-SIRW) in the case $0<p \leq \frac{1}{2}$. The approach is to carefully approximate the local drifts of the random walk via the study of the directed edge local times, which are described by branching-like processes and generalized Ray-knight Theorems. Xiaoyu Liu and Zhe Wang are working on this project. 
		\TBD
	\end{abstract}
	
	\underline{\textsf{Information about coloring:}}
	\begin{itemize}
		\item 
	\textsf{\color{red} Red text, include \TBD marks, are pending works and existing problems.}
		\item 
			\textsf{\color{blue} Blue texts are comments or tentative ideas that can be discussed.}
	\end{itemize}



	\section{Introduction}
	The current project is a completion of Theorem 1.2 in \cite{KMP22}. 
	\TBD 
	
	\subsection{Model Description} 
	We consider a discrete time nearest neighbor self-interaction random walk $(X_i)_{i\geq 0}$ on $\mathbb{Z}$ starting from site $x=0$ with transition probabilities depending on its edge local times and a (deterministic) weight function $w$ on $\mathbb{N}\cup \{0\}$. More precisely, let $w(.)$ be a monotone positive function on $\mathbb{N}_0$ satisfying
	
	
	\begin{equation}\label{eq: asymptotics of w}
		\frac{1}{w(n)} = 1+\frac{2^p B}{n^p} + O\left(\frac{1}{n^{1+\mathcal{\kappa}}}\right), \quad \mbox{as $n\to \infty$}, 	
	\end{equation} 
	The SIRW $(X_i)_{i\geq 0}$ has following dynamics, 
	$X_0 = 0$, and for any $n\geq 0$
	\begin{equation}\label{dynamic}
		\mathbb{P}\left( X_{n+1} =  X_n +1 | X_0,X_1,\dots,X_n   \right) =  \frac{  w(r(X_i,i) )}{ w(l(X_i,i))  + w(r(X_i,i))   },
	\end{equation}
	where $l(x,i)$ and $r(x,i)$ are the local times by time $i$ for the undirected edges $\{x,x-1\}$ and $\{x,x+1\}$
	$$ l(x,i) = \sum_{j=0}^{i-1} \mathbb{1}_{ \{  \{X_j, X_{j+1}\} =  \{x,x-1\} \} }, \mbox{ and }  r(x,i) = \sum_{j=0}^{i-1} \mathbb{1}_{ \{  \{X_j, X_{j+1}\} =  \{x,x+1\} \} }.        $$
	
	In our case, the weight function $w(.)$ is deterministic and identical for each site $x\in \mathbb{Z}$. \TBD (Add backgrounds on the model assumptions, especially why we use deterministic monotone function $w$, and why this is called asymptotically free case. )
	
	\subsection{Main Result}
	\begin{definition}
		Let $\theta_+, \theta_- <1$. A BMPE $W^{\theta_+, \theta_-} = \left(W^{\theta_+, \theta_-}_t, t\geq 0\right)$ with parameter  $(\theta_+, \theta_-)$ is the pathwise unique solution of the equation
		$$
		W_t = B_t + \theta_+ \cdot \sup_{s\leq t} W_s  + \theta_- \cdot \inf_{s\leq t} W_s,   \quad W_0 = 0.
		$$
	\end{definition}
	When the weight function $w(.)$ is monotone and satisfying \eqref{eq: asymptotics of w}, for any $p\in (0,1]$
	\begin{equation}\label{eq: gamma}
		\gamma:= \lim_{n\to \infty}\left( V_1(n) - U_1(n) \right) =\lim_{n\to \infty} \left( \sum_{j=0}^{n-1} \frac{1}{ w(2j+1)}-  \sum_{j=0}^{n-1}  \frac{1}{w(2j)} \right) 
	\end{equation}
	is well defined and $\gamma<1$. It is shown, in Theorem 1.2 \cite{KMP22}, that when $p\in (\frac{1}{2},1]$, and $\kappa >0 $, the rescaled process
	$$
	\left(  \frac{X_{\lfloor nt \rfloor }}{\sqrt{n}}  \right)_{t\geq 0} \Longrightarrow \left( W^{\gamma,\gamma}_{t}\right)_{t\geq 0},
	$$ as $n$ goes to infinity in the standard Skorohod topology $D([0,\infty) ).$
	
	Our main result is the functional limit theorem in the case when $p\in (0,\frac{1}{2})$.
	\begin{theorem}\label{thm: main}
		Let $w(.)$ be monotone and satisfy \eqref{eq: asymptotics of w} for $p\in (0,\frac{1}{2}]$, and $\kappa >0 $. Consider the SIRW $(X_n)_{n\geq 0}$ defined in \eqref{dynamic} with $X_0 =0$. Then the rescaled process
		$$
		\left(  \frac{X_{\lfloor nt \rfloor }}{\sqrt{n}}  \right)_{t\geq 0} \Longrightarrow \left( W^{\gamma,\gamma}_{t}\right)_{t\geq 0},
		$$ as $n$ goes to infinity in the standard Skorohod topology $D([0,\infty) ).$
	\end{theorem}
	The proof of Theorem \ref{thm: main} follows a strategy similar to that of Theorem 1.2 \cite{KMP22}, which relies on martingale methods, analysis of branching-like processes, and generalized Ray-Knight Theorems. This approach is classical in random walk in random environments \TBD, and it has also been used previously in other self-interacting random walk in dimension one, such as \TBD. The novelty of the current article is an approximation of accumulated local drifts $\Delta_y^{(x,m)}$, defined in section \ref{sec: proof of main} below on certain good events, which occurs with a high probability. On these good events, the accumulated local drifts are well approximated by their conditional means $\rho_{y}^{(x,m)}$ given certain edge local times, while their conditional means $\rho_{y}^{(x,m)}$ are close to the $\gamma \cdot sgn(y)$ when certain edge local time is large. \TBD These good events are closely related to the generalized P\'{o}lya  urn processes associated to sites, and we estimtate their probabilities by studying branching-like processes, which are derived from the original SIRW $(X_n)_{n\geq 0}$ via the generalized Ray-Knight Theorem. At this point, we also point out that the approximation of accumulated local drifts $\Delta_y^{(x,m)}$ is different from those in \cite{KMP22} since in the case when $p\in(\frac{1}{2},1]$, the conditional mean $\rho_{y}^{(x,m)}$ converges absolutely, which does not hold in the case when $p\leq \frac{1}{2}$. We need to construct certain good events on which the errors $\Delta_y^{(x,m)}- \rho_y^{(x,m)}$ are well-behaved.
	
	\subsection{Organization of Paper}
	In section \ref{sec: proof of main}, we prove Theorem \ref{thm: main} in several technical steps, each of which is stated as an individual proposition. In particular, the proofs of three technical results are postponed to section \ref{sec: approximations} because they involve analysis of auxiliary processes. In section \ref{sec: generalized Polya Urn, BLP}, we describe these auxiliary processes, generalized P\'{o}lya urn processes and branching-Like processes, discuss their connections to the SIRW $(X_n)$, and show some preliminary results related to technical results. In section \ref{sec: approximations}, we show the approximations of local drifts on good events as well as estimating their probabilities. 
	
	\subsection{Notation}
	We end this section by introducing some notations related to SIRW $(X_n)_{n\geq 0}$. To avoid confusion, the notations related to generalized P\'{o}lya urn, and branching-like processes are in section \ref{sec: generalized Polya Urn, BLP}.
	
	For an SIRW $(X_n)_{n\geq 0}$,  let $(\mathcal{F}^X_n)_n$ be its natural filtration $\mathcal{F}^X_n = \sigma\left(X_i: i\leq n \right).$ We denote by $\mathbb{P}$ and $\mathbb{E}$ its probability and corresponding expectation. The following (random) quantities related to the SIRW $(X_n)_{n\geq0}$ are important in our analysis:
	\begin{enumerate}
		\item for any $x \in \mathbb{Z}$, and $j\in \mathbb{N}_0$, the local time of a site $x$ by time $j$, $L(x,j):= \sum_{i=0}^j \mathbb{1}_{\{X_i=x\} }$; % the definition for local time in \cite{KMP22} and \cite{KP16} are different by at most 1. The main difference comes from whether to count the last step $X_n$. For the former case, $L(x,\lambda_{x,m}) = m+1$, while $L(x,\lambda_{x,m}) =m$ for the latter case.  
		
		\item the local time of directed bond $(x,x+1)$ by time $j\in \mathbb{N}_0$,
		$ \mathcal{E}^j_{x,+} = \sum_{i=0}^{j-1} \mathbb{1}_{\{X_i=x, X_{i+1} =x+1 \} } $; the local time of directed bond $(x,x-1)$ by time $j$ is $ \mathcal{E}^j_{x,-} = \sum_{i=0}^{j-1} \mathbb{1}_{\{X_i=x, X_{i+1} =x-1 \} }; $
		
		
		\item for any $m\geq 0$, the \edt{$m+1$}-th visit time to site $x$, $\lambda_{x,m} = \inf\{t \geq 0: L(x,t) = \edt{m+1} \}$;
			\comment{A possible notation scheme: $L_y^k$ for site local time; $\overleftarrow{L}_y^k = L_{y, -}^k$ for directed edge local time $y \to y-1$; $\overrightarrow{L}_y^k = L_{y, +}^k$ for directed edge local time  $y \to y+1$; $L_{\{y, y+1\}}^k $ for undirected edge local times.}
		
		\item the running maximum of the process at time $n$, $M_n= \sup\{y: L(y,n)\geq 1 \} $; the minimum of the process at time $n$, $I_n= \inf\{y: L(y,n)\geq 1 \} $;
		
		\item 
%		the accumulated local drifts at site $x$ by $m$-th visit, $$\delta_{x,m}:= \sum_{i=0}^\infty E[X_{i+1}-X_i\vert \mathcal{F}_{i}^X] \mathbb{1}_{\{X_i=x, L(x,i)\leq m\}};$$ 
		the accumulated local drifts at site \edt{$y$} by time $\lambda_{x,m}$, 
		\begin{equation}\label{eq: accumulated local drift}
			\Delta_y^{(x,m)}:= \sum_{i=0}^{\lambda_{x,m}-1} E[X_{i+1}-X_i\vert \mathcal{F}_{i}^X] \mathbb{1}_{\{X_i=y\}}.
		\end{equation}
	\end{enumerate}
	
	\section{Outline for the Proof of Theorem \ref{thm: main}}\label{sec: proof of main}
	The proof of Theorem \ref{thm: main} follows a classical strategy in obtaining functional limit theorems. We will divide the proof in four main steps. We first decompose the random walk into
	$$X_n = M_n+ \Gamma_n, $$ where
	$$ 
	\Gamma_0 = 0, \quad \Gamma_n = \sum_{i=0}^{n-1} \mathbb{E}\left[ X_{i+1}-X_i | \mathcal{F}_i^X \right].
	$$ 
	
	\textbf{Step 1: Control of martingale term.}
	The choice of $\Gamma_n$ allows us to get a martingale $M_n$ with respect to $\mathcal{F}_i^X.$ Then using the Martingale Central Limit Theorem (Theorem~18.2, \cite{B99}), the rescaled process $\frac{M_n}{\sqrt{n}}$ converges to a standard brownian motion in distribution if we have the control of martingales 
	\begin{equation}\label{eq: QV term}
		\lim_{n\to \infty}\frac{1}{n} \sum_{i=0}^{n-1}\mathbb{E}\left[ (M_{i+1}- M_{i})^2 |\mathcal{F}_i^X \right] =1,  \mbox{ in probability}.
	\end{equation}
	Since $\abs{X_{n+1}-X_n}=1$,  \eqref{eq: QV term} is implied by the estimate
	\begin{proposition} \label{lm: control of martingale} Let $p\in (0,\frac{1}{2}]$. Then, for any $\epsilon >0$
		\begin{equation}\label{eq:  term}
			\lim_{n \to \infty }\mathbb{P}\left(\frac{1}{n} \sum_{i = 0}^{n-1} \mathbb{E}\left[ X_{i+1} - X_i | \mathcal{F}_i \right]^2 > \epsilon \right) =0. 
		\end{equation}
	\end{proposition}
	The proof of Proposition \ref{lm: control of martingale} is in section \ref{sec: approximations}.
	
	\textbf{Step 2: Control of accumulated drift.} This is the major technical step of our article. We want to approximate the accumulated drift $\Gamma_k$ by a fixed linear combination of the distances between $X_k$ and its running maximum $S_k$ and minimum $I_k$:
	\begin{proposition}\label{lm: control of acc drift}
		Let $p\in (0,\frac{1}{2}]$. Then, for any $t>0$ and any $\epsilon >0$
		\begin{equation}\label{eq: control of acc drift}
			\lim_{n \to \infty }\mathbb{P}\left(\sup_{k\leq nt} \abs{\Gamma_k - \gamma \left(M_k + I_k \right)   } > \epsilon \sqrt{n}  \right) =0. 
		\end{equation}
	\end{proposition}
	To prove Proposition \ref{lm: control of acc drift}, we \edt{first decompose the accumulated drift $\Gamma_k$ as $\Gamma_k = 	\Gamma_k^+ +	\Gamma_k^0 + \Gamma_k^-$, where} 
	\begin{align*}
		\Gamma_k^+ &= \sum_{y > X_k} \sum_{i = 0}^{k-1} \mathbb{E}\left[ X_{i + 1} - X_i | \mathcal{F}_i^X \right] \mathbf{1}(X_i = y)\\
		\Gamma_k^0 &= \sum_{i = 0}^{k-1} \mathbb{E}\left[ X_{i + 1} - X_i | \mathcal{F}_i^X \right] \mathbf{1}(X_i = X_k) \\
		\Gamma_k^- &= \sum_{y < X_k} \sum_{i = 0}^{k-1} \mathbb{E}\left[ X_{i + 1} - X_i | \mathcal{F}_i^X \right] \mathbf{1}(X_i = y)
		.\end{align*} 
	 We only need to consider the term $\Gamma_k^+$ while the contribution from $\Gamma_k^-$ follows a symmetric argument. And with a similar argument, we get that the contribution from $\Gamma_k^0$ is negligible (\TBD).
	 \edt{
	 Then,  we consider the stopping time $\lambda_{x,m}$, for any integers $x ,m  \in \mathbb{Z}$ with $m \geq 0$. The event that 
	 $$\{ \lambda_{x,m} =k \} = \{ X_k = x, L(x,k) =m +1  \}.  $$ 
	In view of \eqref{eq: accumulated local drift},  $\Gamma_k^+$ is the sum} 
	\begin{equation}
	\Gamma_k^+ = \sum_{y>x} \Delta_y^{(x,m)}, \mbox{\edt{\quad on the event that $\{ \lambda_{x,m}=k  \}$}},
	\end{equation} 
	which has $(S_k - X_k)$ terms. If we can further approximate terms $\Delta_y^{(x,m)}$ by $\gamma\cdot sgn(y)$ without generating a total error of size $\epsilon \sqrt{n}$, $\Gamma_k^+$ is approximately 
	$$   
	\gamma \cdot (S_k - \abs{X_k}).
	$$
	More precisely, \edt{we want to show that} for any $t>0$ and any $\epsilon >0$
	\begin{equation}\label{eq: control of acc drift + }
		\lim_{n \to \infty }\mathbb{P}\left(\sup_{k\leq nt} \abs{\Gamma^+_k - \gamma \cdot \left(M_k - \abs{X_k} \right)   } > \epsilon \sqrt{n}  \right) =0. 
	\end{equation}
	To this end, we consider the filtration $\left(\mathcal{G}_{y}^{(x,m)}\right)_{y\geq x}$, where $ \mathcal{G}_{y}^{(x,m)} = \sigma\left( \mathcal{E}^{(x,m)}_{y,+} : y \geq x \right)$, and further approximate $\Delta_y^{(x,m)}$ by its conditional expectation with respect to $\mathcal{G}_{y-1}^{(x,m)}$,
	\begin{equation}\label{eq: conditional mean}
		\rho_{y}^{(x,m)}= \mathbb{E}\left[\Delta_y^{(x,m)} | \mathcal{G}_{y-1}^{(x,m)}\right].
	\end{equation}
	Then \eqref{eq: control of acc drift + } follows from the following two propositions:
	\begin{proposition}\label{lm: approximation of means of local drift}
		Let $p\in (0,\frac{1}{2}]$. Then, for any $t>0$ and any $\epsilon >0$
		\begin{equation}\label{eq: control of expected local drift}
			\lim_{n \to \infty }\mathbb{P}\left(\sup_{k\leq nt} \abs{\sum_{y> X_k} \left( \rho_{y}^{\edt{(X_k,L(X_k,k))}} - \gamma  \cdot sgn(y) \right)   }  > \epsilon \sqrt{n}  \right) =0. 
		\end{equation}
	\end{proposition}
	and
	\begin{proposition}\label{lm: approx local drift by conditional means}
		Let $p\in (0,\frac{1}{2}]$. Then, for any $t>0$ and any $\epsilon >0$
		\begin{equation}\label{eq: control of martingale difference for local drift}
			\lim_{n \to \infty }\mathbb{P}\left(\sup_{k\leq nt} \abs{\sum_{y> X_k} \left(\Delta_{y}^{\edt{(X_k,L(X_k,k))}}- \rho_{y}^{\edt{(X_k,L(X_k,k))}} \right)   }  > \epsilon \sqrt{n}  \right) =0. 
		\end{equation}
	\end{proposition}

	\edt{Consider changing both propositions to a version for $(x,m)$. This allows us to unify the part of the proof where we go from $(X_k, L(X_k, k))$ to $(x,m)$.}

	To prove both Propositions \ref{lm: approximation of means of local drift} and \ref{lm: approx local drift by conditional means}, we will introduce auxiliary processes associated to the SIRW $(X_n)_{n\geq 0}$: the generalized P\'{o}lya Urn process, and branching-like processes. Both processes are natural projections of $(X_n)_{n\geq 0}$ on certain stopping times, and they aid our approximations of $\Delta_{y}^{(x,m)}$ on certain good events. The good events in the proof of Proposition \ref{lm: approximation of means of local drift} are easier because events like Lemma \ref{lm: number of rarely visit sites} below will almost do the job, and estimates of their probabilities are essentially known from \cite{KMP22}. However, the good events for Proposition \ref{lm: approx local drift by conditional means} requires a new construction, and estimating the error requires a new proof, which is the major novelty of this article. We will postpone the proofs of Propositions \ref{lm: approximation of means of local drift} and \ref{lm: approx local drift by conditional means} together with the constructions of good events to section \ref{sec: approximations} after introducing the generalized P\'{o}lya Urn process, and branching-like processes in section \ref{sec: generalized Polya Urn, BLP}.
	
	\TBD With some slight efforts, we extend \eqref{eq: control of acc drift + } for $\Gamma_k^+$ to $\Gamma_k^-$, and obtain the full Proposition \ref{lm: control of acc drift}.
	
	
	\textbf{Step 3: Tightness.} \TBD This follows from (\cite{KMP22}, Proposition~2.1).
	
	\textbf{Step 4: Convergence to BMPE.} \TBD The proof follows word by word.
	
	\section{Generalized P\'{o}lya Urn, Branching-Like Processes}\label{sec: generalized Polya Urn, BLP}

	In this section, we describe two auxiliary processes, generalized P\'{o}lya urn processes and branching-like processes, and recall some of their properties needed in the proofs of Propositions \ref{lm: control of martingale}, \ref{lm: approximation of means of local drift}, and \ref{lm: approx local drift by conditional means}. Both processes can be obtained from the SIRW $(X_n)_{n\geq 0}$ at stopping times. We start with the generaized P\'{o}lya urn processes. 

	\subsection{Generalized P\'{o}lya  Urn}
	Given a (recurrent) random walk $(X_n)_{n\geq 0}$, and a fixed site $y\in \mathbb{Z}$, we can obtain a Markov process by considering only the up-crossings and down-crossings of $X_n$ from site $y$. More precisely, let $(\lambda_{y,k})_{k\geq 0}$ be the stopping times when $X_n$ \edt{visit site $y$ for the $\left( k+1 \right) $-th time:}
	\[
		\lambda_{y,0} :=\inf\{ k\geq 0: X_k = y \} , \quad \lambda_{y,k+1} := \inf\{ t> \lambda_{y, k}: X_t = y \}.
	   \] 
	
	   \edt{which is consistent with the notation in point 3 of subsection 1.4. }
	   We define the \textit{generalized P\'olya urn process} at site $y$ as 
	  \begin{equation} \label{eq: RW to GPU}
	   \left(\mathcal{B}^{(y)}_{k},\mathcal{R}^{(y)}_{k} \right)_{k\ge 0}
	   :=\left(\mathcal{E}^{\lambda_{y,k}}_{y,-}, \mathcal{E}^{\lambda_{y,k}}_{y,+}\right)_{k\geq 0} 
	   =  \left(\mathcal{E}^{(y,k)}_{y,-}, \mathcal{E}^{(y,k)}_{y,+}\right)_{k\geq 0},
	   \end{equation}
	   which is a Markov process with an initial value $(0,0)$. \comment{We can use $k$ only in the context of $X_k$.} Intuitively, $\left(\mathcal{B}_{i}^{(k)},\mathcal{R}_{i}^{(y)} \right)$ is the state of a (generalized) P\'olya urn, after the $i$-th draw made from the urn. This is the reason that we define $\lambda_{y, 0}$ to be the first time the walk reaches $y$ : this is right before the first draw be made from the urn. 
	\begin{remark}
		\label{rem:symmetry}
		By symmetry considerations, when $X_0 = 0$, $\mathcal{E}_{y, + }^{k} \overset{d}{=} \mathcal{E}_{-y, - }^{k}$ for all $k \ge 0$ and $y \in \mathbb{Z}$. Moreover, $\mathcal{E}_{y, +}^{(x, m)} \overset{d}{=} \mathcal{E}_{-y, -}^{(-x, m)}$ for all $x, y \in \mathbb{Z}$.
\[
		 \left(\mathcal{E}^{(x,m)}_{x+k,+} \right)_{k\geq 0} \overset{d}{=} \left(\mathcal{E}^{(-x,m)}_{-x-k,-} \right)_{k\geq 0}
		\]
	   \end{remark}

	   
	 
	   Due to the initial value of the underlying random walk $X_0=0$, the local times of undirected edges $\{y,y-1\}$ and $\{y,y+1\}$ at time $\lambda_{y,0}$ is 
	\begin{equation}\label{eq: initial condition}
		\left(l(y,\lambda_{y,0}),  r( y ,\lambda_{y,0})\right) =  \begin{cases}	
			(1, 0) &,  \text{ if }  y>0 \\
			(0, 1) &,  \text{ if }  y<0 \\  
			(0, 0) &,  \text{ if }  y=0 \\
		\end{cases} 
	.\end{equation}	
	Therefore, we get three types of transition probabilities
	for the generalized P\'{o}lya urn process depending on $y>0$, $y<0$ or $y=0$:
	\begin{align*}\label{eq: transition prob for GPU}
		\mathbb{P} \left(\left(\mathcal{B}_{k+1,y},\mathcal{R}_{k+1,y} \right)=  (i+1,j) \vert (\mathcal{B}_{k,y},\mathcal{R}_{k,y}) =(i,j)  \right) &= \frac{b(i)}{b(i)+r(j)}, \mbox{ and}  \\
		\mathbb{P} \left((\mathcal{B}_{k+1,y},\mathcal{R}_{k+1,y})=  (i+1,j) \vert (\mathcal{B}_{k,y},\mathcal{R}_{k,y}) =(i,j)  \right) &= \frac{r(j)}{b(i)+r(j)},
	\end{align*} 
	where the generalized weights $(b(k),r(k))_{k\geq 0}$ depend on $w(.)$ and $y$ by  
	\begin{equation}\label{eq: generalized weights}
		(b(k), r(k)) = \begin{cases}
			(w(2k+1), w(2k)) &,  \text{ if }  y>0 \\
			(w(2k), w(2k+1)) &,  \text{ if }  y<0 \\  
			(w(2k), w(2k)) &,  \text{ if }  y=0 \\ 
		\end{cases}.
	\end{equation}
	For simplicity of notation, we drop the subscript $y$ when a fixed site of $y$ is clear from context. For the same reason, we write $\tau_k^{\mathcal{B}}$ in place of $\tau_k^{\mathcal{B}^{(y)}}$, and $\tau_k^{\mathcal{R}}$ in place of $\tau_k^{\mathcal{R}^{(y)}}$.

	For a generalized P\'{o}lya urn process $(\mathcal{B}_k,\mathcal{R}_k )_{k\geq 0}$ associated to the site $y$, we define the signed difference process $\{\mathcal{D}_{k}\}_{k \ge 0} $ to be
	\begin{equation}\label{eq:signed difference}
		\mathcal{D}_k  =\mathcal{R}_k -\mathcal{B}_k.  
	\end{equation}
	Also, for every integer $k\geq 0$, we denote by $\mathcal{F}^{\mathcal{B},\mathcal{R}}_k$ (or $\mathcal{F}^{\mathcal{B},\mathcal{R}}_{k,y}$) the sigma algebra generated by  
	\[\mathcal{F}^{\mathcal{B},\mathcal{R}}_k = \sigma\left((\mathcal{B}_j,\mathcal{R}_j ): j\leq k \right).
	\]  
	
	\TBD With the generalized P\'{o}lya urn process $(\mathcal{B}_k,\mathcal{R}_k )_{k\geq 0}$ associated to the site $y$, we can study $\Delta_{y}^{(x,m)}$ and its approximation $\rho_{y}^{(x,m)}$ defined in \eqref{eq: accumulated local drift} and \eqref{eq: conditional mean}. For $y>x $, at time $\lambda_{x,m}$, the local time at site $y$ is $L(y,\lambda_{x,m})$, and the last jump from site $y$ is a left jump. Then for any integers $k\geq 1$ and $k' \geq 0$, the event 
	$$\left\{ \mathcal{E}^{(x,m)}_{y-1,+} =k, \mathcal{E}^{(x,m)}_{y,+} = k'\right\} = \left\{\mathcal{E}^{(x,m)}_{y,-} =k,  L(y,\lambda_{x,m}) = k+k' \right\} = \{ \tau^B_{k,y} = k+k' \},$$ on which the (random) quantity $\Delta_{y}^{(x,m)}$ equals


	\[
	\Delta_{y}^{(x,m)} =\sum_{j=0}^{ L(y,\lambda_{x,m})-1} \mathbb{E}\left[ \mathcal{D}_{j+1} -\mathcal{D}_{j}  \vert \mathcal{F}^{\mathcal{B},\mathcal{R}}_{j} \right] = \sum_{j=0}^{\tau^B_{k,y}-1} \mathbb{E}\left[ \mathcal{D}_{j+1} -\mathcal{D}_{j}  \vert \mathcal{F}^{\mathcal{B},\mathcal{R}}_{j} \right].  
	\] 
	Summing over the terms between two consecutive stopping times $\tau^{B}_{l,y} $ and $\tau^{B}_{l+1,y} $, for any $0\leq l \leq k -1$,  we get a sum depending only on $\tau^{B}_{l,y} $, $\tau^{B}_{l+1,y} $ and $l$, 
	\begin{align} \label{eq: conditional increment}
		 \sum_{j=\tau^{B}_{l,y}}^{\tau^B_{l+1,y}-1} \mathbb{E}\left[ \mathcal{D}_{j+1} - \mathcal{D}_{j}  \vert \mathcal{F}^{\mathcal{B},\mathcal{R}}_{j} \right] =&
		  \sum_{j=0}^{\tau^B_{l+1,y}-\tau^{B}_{l,y}-1} \frac{ r(\tau^{B}_{l,y}-l + j) - b(l)  }{ r(\tau^{B}_{l,y}-l + j) + b(l)  } 
		  =:  D\left(\tau^{B}_{l,y},\tau^{B}_{l+1,y},l\right),
	\end{align}   
	where $(b(i),r(i))_{i\geq 0}$ are the general weights associated to $y$ from \eqref{eq: generalized weights}.
	Therefore,  $\Delta_{y}^{(x,m)}$ is a sum of $\mathcal{E}^{(x,m)}_{y-1,+} $ increments which (jointly) depend only on $ \left(\tau^{B}_{l+1,y}: 0\leq l\leq   \mathcal{E}^{(x,m)}_{y-1,+}-1 \right) $
			\begin{equation} \label{eq: cummulated drift at a site}
				\Delta_{y}^{(x,m)} = \sum_{l=0 }^{ \mathcal{E}^{(x,m)}_{y-1,+} -1  } D\left(\tau^{B}_{l,y},\tau^{B}_{l+1,y},l \right).
			\end{equation}			
	From \eqref{eq: conditional increment}, $\abs{  D\left(\tau^{B}_{l,y},\tau^{B}_{l+1,y},l\right)} \leq  \tau^{B}_{l+1,y}-\tau^{B}_{l,y}$ is stochastically dominated by a geometric random variable with a mean independent of $l$ and $y$. 
	Since the process $(\tau^B_{l,y})_{l\geq 0}$ is Markov, and  $ (\mathcal{E}^{(x,m)}_{y})_{ y \geq x} $ is also Markov from the following subsection \TBD, we get that, on the event that $ \left\{ \mathcal{E}^{(x,m)}_{y-1,+} \equiv \mathcal{E}^{(x,m)}_{y,-}  = k \right\}$,
	\[
		  \rho_{y}^{(x,m)} := 
		  \mathbb{E}\left[\Delta_{y}^{(x,m)} \vert \mathcal{G}_{y-1}^{(x,m)} \right] 
		= \mathbb{E}\left[ \sum_{l=0 }^{ k -1  }  D\left(\tau^{B}_{l,y},\tau^{B}_{l+1,y},l\right) \vert \mathcal{E}^{(x,m)}_{y-1,+} \right]. 
	\]
	As $\left(\tau^B_{l,y}\right)_{l \geq 0} $ is independent of $ \mathcal{E}^{(x,m)}_{y-1,+} $,  $\left(D\left(\tau^{B}_{l,y},\tau^{B}_{l+1,y},l\right))\right)_{l\geq 0}$ is also independent of $ \mathcal{E}^{(x,m)}_{y-1,+} $. Then, from \eqref{eq: conditional increment} again, on the event $\left\{\mathcal{E}^{(x,m)}_{y-1,+}  = k  \right \}$
	\begin{equation} \label{eq: conditional mean in GPU represenetation}
		\rho_{y}^{(x,m)} = \mathbb{E}\left[ \sum_{l=0 }^{ k -1  }  D(\tau^{B}_{l,y},\tau^{B}_{l+1,y} ) \vert \mathcal{E}^{(x,m)}_{y-1,+} \right]	= \mathbb{E}\left[ \sum_{l=0}^{k-1}  \mathcal{D}_{\tau^{B}_{l+1,y}} -\mathcal{D}_{\tau^{B}_{l,y}} \right]  
		= \mathbb{E}\left[  \mathcal{D}_{\tau^{B}_{k,y}} \right].
	\end{equation} 
	\comment{This needs to be more motivated at the start of the section}
	 Expressions like \eqref{eq: cummulated drift at a site} and \eqref{eq: conditional mean in GPU represenetation} help us simplify the analysis of $\Delta_{y}^{(x,m)}$ and $\rho_{y}^{(x,m)}$, as will be shown section \ref{sec: approximations}. To analyze events like $ L(y,\lambda_{x,m}) = k+k'$ and $\mathcal{E}^{(x,m)}_{y-1,+} = k $, the branching-like processes play a key role.
		
		
		\subsection{Branching-Like Processes}
		Having understood the behavior of the walk at a fixed site in terms of the urn processes, we now want to characterize how urn processes at different sites are related. Unlike the generalized P\'{o}lya urn process $(\mathcal{B}_k,\mathcal{R}_k )_k$ which is a Markov process in time, the branching-like processes is a Markov process in space, and it describes the local times of directed edges of $(X_n)_{n\geq 0}$ at a fixed stopping time.
		More precisely, for any integers $x,m$ with $m\geq 0$, the local times at the stopping time $\lambda_{x,m}$, 
		\[
		 \left(\mathcal{E}^{(x,m)}_{x+k,+} \right)_{k\geq 0}, \quad = \left(\mathcal{E}^{(x,m)}_{x-k,-} \right)_{k\geq 0}
		\]
		are two Markov processes on $\mathbb{N}\cup\{0\}$, whose transition probabilities are related to the generalized weights in \eqref{eq: generalized weights}.
		
		
		To derive the transition probabilities, we first use Remark~\ref{rem:symmetry} and assume $x \ge 0$. We define the \textit{branching-like processes} to be
		\[
			\tilde{\zeta} := \left(\mathcal{E}^{(x,m)}_{x+k,+} \right)_{k\geq 0}, \quad
			\zeta := \left(\mathcal{E}^{(x,m)}_{x-k,-} \right)_{k\geq 0}
			.\]
			We start with $\tilde{\zeta}$ which has transition probabilities homogeneous in k, while $\zeta$ has inhomo
			geneous ones. Although the generalized P\'{o}lya urn processes at different sites are not independent, the sequences $(\tau^B_{k,y})_{k\geq 0} $ are independent in $y \geq x$. As each sequence $\left(\tau^B_{k,y}\right)_{k\geq 0} $ is Markov in $k$,  the collection of stopping times  
			\begin{equation}\label{eq: markov 1} 
				\left\{\tau^B_{k,y}: y\geq x, k\geq 0 \right\} \mbox{are Markov in $(y,k)$}
			\end{equation}
			under the lexicographical order (which is a total order on any subset of $\mathbb{Z}^2$): 
			\begin{equation}\label{eq: lexicographical order}
				(y,k) \preceq (y',k')  \mbox{ if and only if  }
				k \leq k'   \mbox{ when $y' = y$,  or }   
				y <y'. 
			\end{equation} 
			Moreover, at time $\lambda_{x,m}$, for any site $y\geq x$, we only need the indices 
			$$
			k\leq \mathcal{E}_{y,-},
			$$
			and $\mathcal{E}_{y+1,-}$ can be obtained recursively from $\mathcal{E}_{y,-}$ and $(\tau^B_{k,y})_{k}$. Since the last jump from site $y+1$ is a left jump and it is the $\mathcal{E}_{y+1,-}^{(x,m)}=\mathcal{E}_{y,+}^{(x,m)}$ -th left jump, the number of right jumps from site $y+1$ is exactly
		\begin{equation} \label{eq: recursive formula for upcrossings}
			\mathcal{E}_{y+1,+}^{(x,m)}	=  \sum_{k= 0 }^{\mathcal{E}_{y+1,-}^{(x,m)}-1}	\left(\tau^B_{k+1,y}-\tau^B_{k,y}-1 \right) = \tau^B_{ L,y } - L = \mathcal{R}_{\tau^B_{ L,y }},
		\end{equation}  
	   where $L = \mathcal{E}_{y+1,-}^{(x,m)}=\mathcal{E}_{y,+}^{(x,m)} $. With \eqref{eq: markov 1}, and \eqref{eq: recursive formula for upcrossings}, we conclude that $\tilde{\zeta} = \left(\mathcal{E}^{(x,m)}_{x+k,+} \right)_{k\geq 0}$ is a homogeneous Markov chain when $x\geq 0$, and its the transition probabilities are 
	   	\begin{equation}\label{eq: transition prob on positive}
	   	\mathbb{P}\left(\tilde{\zeta}_{k+1}=j \vert \tilde{\zeta}_k =i  \right) = 
	   	\mathbb{P}\left( \mathcal{R}_{\tau_{i,1}^B} = j \right), \mbox{for any $i,j\geq 0$, } 
	   \end{equation} 
   		where the generalized weights corresponds to the case when $y>0$.
   		For $\zeta= \left(\mathcal{E}^{(x,m)}_{x-k,-} \right)_{k\geq 0}$ follows a similar arguments. We get similar statements as \eqref{eq: markov 1} and \eqref{eq: recursive formula for upcrossings}:
   		\begin{enumerate}
   			\item 
   			at time $\lambda_{x,m}$, for any $y\leq x$, the last jump from site $y-1$ is a right jump, and it is the 
   			\begin{equation}\label{eq: source of inhomogeneity}
   				\mathcal{E}_{y-1,+}^{(x,m)} = \mathcal{E}_{y,-}^{(x,m)} + \mathbb{1}_{ \{ 1\leq y \leq x \} }
   			\end{equation}
   			-th right jump; 
   			\item 
   			the collection of stopping times
   			\begin{equation}\label{eq: markov 2} 
   				\left\{\tau^R_{k,x-y}: y\geq x, k\geq 0 \right\} \mbox{is Markov in $(y,k)$}
   			\end{equation} with respect to the lexicographical order \eqref{eq: lexicographical order}; 
   			\item 
   			for $y\leq x$, the number of left jumps from site $y-1$  at time $\lambda_{x,m}$ is
   		\begin{equation} \label{eq: recursive formula for downcrossings}
   			\mathcal{E}_{y-1,-}^{(x,m)} 	=  \sum_{k= 0 }^{\mathcal{E}_{y-1,+}^{(x,m)}-1}	\left(\tau^R_{k+1,y}-\tau^R_{k,y}-1 \right) = \tau^R_{ L,y } - L = \mathcal{B}_{\tau^R_{ L,y }},
   		\end{equation}  where $L= \mathcal{E}_{y-1,+}^{(x,m)} = \mathcal{E}_{y,-}^{(x,m)} + \mathbb{1}_{\{ 1\leq y \leq x \}} $ 
   		for the generalized Polya urns at site $y-1$.
   		\end{enumerate}	
   	 Therefore, $\zeta= \left(\mathcal{E}^{(x,m)}_{x-k,-} \right)_{k\geq 0}$ is an inhomogeneous Markov chain, with the transitional probabilities  depending on the sign of $x-k$: for $i,j\geq 0$
	   \begin{equation}\label{eq: transition prob on negative}
		   	\mathbb{P}\left(\zeta_{k+1}=j \vert \zeta_k =i  \right) = 
		   \begin{cases}
		   	\mathbb{P}\left( \mathcal{B}_{\tau_{i+1,1}^R} = j \right) ,& \mbox{ if $0 \leq k <  x-1$ }
		   	\\
		   	\mathbb{P}\left( \mathcal{B}_{\tau_{i+1,0}^R} = j \right) ,& \mbox{ if  $k =  x-1$, }
		   	\\
		   	\mathbb{P}\left( \mathcal{R}_{\tau_{i,-1}^B} = j \right) ,& \mbox{ if $k \geq x$ }
		   \end{cases}
		\end{equation}
 where the generalized weights correspond to the case when $y>0$, $y=0$ and $y<0$ from \eqref{eq: generalized weights}.

It's convenient to consider two types of filtrations for both $\tilde{\zeta}$ and $\zeta$. The natural filtrations of $\tilde{\zeta}$ and $\zeta$ are of the first type. They are defined via $\mathcal{G}_{y, +}^{(x,m)}:=\sigma\left(\mathcal{E}^{(x,m)}_{z, +}: x \le z \le y\right) $ for $y \ge x$ and $\mathcal{G}_{y, -}^{(x,m)}:=\sigma\left(\mathcal{E}^{(x,m)}_{z, -}: y \le z \le x\right) $ for $y \le x$.
The second type of filtration is built from independent Markov processes $\left(\tau_{k, z}^{B}\right)_{k\geq 0}$ and random times $ \mathcal{E}^{(x,m)}_{z, -}$:
\edt{
\[
	\mathcal{H}_{y, +}^{(x,m)} = \sigma\left( \mathcal{E}_{z, -}^{(x,m)}, \tau_{k, z}^{B}\cdot \mathbb{1}_{\{ k\leq \mathcal{E}_{z, -}^{(x,m)} \}} : x \leq  z \leq y,  k \geq 0 \right) 
.\] 
} 
for $y\geq x$, and $\mathcal{H}_{y, -}^{(x,m)}$ is defined similarly for $y\leq x$.
In particular, $\mathcal{E}_{y+1, -}^{(x,m)} = \mathcal{E}_{y, +}^{(x,m)}$ is  $\mathcal{H}_{y, +}^{(x,m)}$- measurable for any $y\geq x$ by \eqref{eq: recursive formula for upcrossings}, and similarly, $\mathcal{E}_{y, -}^{(x,m)}$ is $\mathcal{H}_{y, -}^{(x,m)}$- measurable for $ y\leq x$ by \eqref{eq: source of inhomogeneity} and \eqref{eq: recursive formula for upcrossings}. Therefore, we get a finer filtration than the first type.
 We drop the $+, -$ signs when there is no ambiguity. Now, observe that 
\begin{enumerate}
	\item 
$\mathcal{G}_y^{(x,m)} \subset \mathcal{H}_y^{(x,m)}.$ 
	\item $\Delta_y^{(x,m)} \in \mathcal{H}_y^{(x,m)}$ but $\not\in \mathcal{G}_y^{(x,m)}$
	\item $\rho_y^{(x,m)} \in \mathcal{G}_y^{(x,m)}$.
\end{enumerate}
		
		The construction of branching-like processes enables us to study the local times profiles at a generic local $\lambda(x,m)$, from the Markov processes $\zeta$ and $\bar{\zeta}$. An advantage is that a typical event (\TBD{see equations to be defined in section \ref{sec: approximations}}) on a collection of spatial points will be also a typical event on a 'typical' single site. Then expressions like \eqref{eq: conditional mean in GPU represenetation} reduces the problem to a problem involving generalized P\'{o}lya urn process associated to a single site. The difficulty is then transferred to constructing typical events that can be estimated. For example, Lemma \ref{lm: number of rarely visit sites} below describes a typical event, and it reduces the proof of proposition \ref{lm: approximation of means of local drift} to \eqref{eq: convergence of conditional expectation} below. In the following subsection, we recall some properties of generalized P\'{o}lya urn process and branching-like processes.
		
		\subsection{Preliminary Results}
		To facilitate our arguments in section \ref{sec: approximations}, we list four results from \cite{KMP22,T96}, whose proofs \textcolor{red}{we omit at this moment.}
		
		The first result is a concentration inequality for $\mathcal{D}_i$ in a generalized P\'{o}lya urn process. This lemma is a major tool in estimating the probabilities of good events.
		\begin{lemma}(Lemma 4.1 \cite{KMP22})\label{lm: concentration inequality}
			Let weights $r(i) = w(2i)$, $b(i)= w(2i+1) $ for all $i\geq 0$. Then there exists constants $C,c>0$ such that for $k, m \in \mathbb{N}$,
			$$
			P\left(  \abs{ \mathcal{D}_{\tau_k^B}   } \geq m \right) \leq C e^{\frac{-cm^2}{m \vee k}}.
			$$
		\end{lemma} 
		Lemma \ref{lm: concentration inequality} remains valid for the generalized P\'{o}lya urn process $(\mathcal{B}_{k},\mathcal{R}_{k})_k$ associated to sites $y<0$ and $y=0$. For these two cases, the sequence of weights $(r(i),b(i))$ are slightly different, see \eqref{eq: generalized weights}. When $y<0$, $r(i) = w(2i+1)$, $b(i)= w(2i) $; when $y=0$, $r(i) = b(i)=w(2i)$.
		
		The second result is an identity for a generalized P\'{o}lya urn process $(\mathcal{B}_{k},\mathcal{R}_{k})_k$. For any $k\geq 0$ denote by $\mu(k)= \tau^B_k - k$ the number of red balls extracted before the $k$-th blue ball. 
		\begin{lemma}(Lemma 1, \cite{T96}) \label{lm: Toth's Identity}
			For any $m\in \mathbb{N}$ and $\lambda < \min\{ b(j): 0\leq j\leq m-1 \}$, we have the following identity,
			$$  \mathbb{E}\left[  \prod_{j=0}^{ \mu(m)-1 } \left(1+ \frac{\lambda}{r(j)}   \right) \right] =   \prod_{j=0}^{ m-1 } \left(1- \frac{\lambda}{b(j)}   \right)^{-1}.   $$ 
			In particular, 
			\begin{equation}\label{eq: Toth's Identity 1}
				\mathbb{E}\left[  \sum_{j=0}^{ \mu(m)-1 } \frac{1}{r(j)}   \right] =   \sum_{j=0}^{ m-1 } \frac{1}{b(j)}.
			\end{equation}	
		\end{lemma}
		\eqref{eq: Toth's Identity 1} is a direct consequence of the first identity. And the first identity can be proved via (exponential) martingales associated to the generalized P\'{o}lya urn process $(\mathcal{B}_{k},\mathcal{R}_{k})$, 
		$$M_k(\lambda) = \prod_{i=0}^{ \mathcal{B}_{k}-1 } \left(1-\frac{\lambda}{b(i)}\right) \prod_{j=0}^{\mathcal{R}_{k}-1 } \left(1+\frac{\lambda}{r(j)}\right). $$
		
		The third result is about the diffusion approximations of the branching-like processes, and it follows from the Proposition A.3 in \cite{KMP22}. Its proof follows arguments from Toth \cite{T96}. A consequence of this result is the process level tightness of extrema, Proposition 2.1 \cite{KMP22}. 
		
		\begin{lemma}(Proposition A.3 \cite{KMP22})\label{lm: diffusion approximation of blp}
			For $n\geq 1$, let $\zeta^{(n)}=(\zeta^{(n)}_k)_{k\geq 0 }  $ be the BLP with initial value $\zeta^{(n)}_0 = \lfloor yn \rfloor$ for some $y \geq 0$, and let $\mathcal{Z}_n(t) = \frac{\zeta^{(n)}_{\lfloor nt \rfloor}}{n}$ for $n\geq 1$ and $t\geq 0$. Then we have that 
			$$
			\mathcal{Z}_n(.) \Longrightarrow Z^{(2-2\gamma)}(.)
			$$ as $n$ goes to infinity on $D([0,\infty)),$ where $Z^{(2-2\gamma)}(.)$ is the squared Bessel processes of dimension $2-2\gamma$.
		\end{lemma}

		
		The last result is a result from the branching-like processes. It gives a control of the number of sites with "small" local times:
		\begin{lemma}(Lemma 2.2 \cite{KMP22})\label{lm: number of rarely visit sites}
			Let $\gamma_+ = \gamma \vee 0$. Then for any $M>0$, and any $b>\frac{\gamma_+}{2}$ we have
			$$
			\lim_{n\to\infty} P\left(\sup_{k\leq nt}  \sum_{x\in [I^X_{k-1}, S^X_{k-1}]} \mathbb{1}_{\{ L(x,k-1) \leq M \}} \geq 4n^b \right) = 0.
			$$
			
		\end{lemma}	
		Lemma \ref{lm: number of rarely visit sites} is a technical result, and its proof involves the analysis of BLPs and the concentration inequality in Lemma \ref{lm: concentration inequality} for the generalized P\'{o}lya urn process. The statement of Lemma \ref{lm: number of rarely visit sites} remains in force if we replace the range $[I_{k-1}, M_{k-1}]$ by $[X_k,M_k]$ (or $[I_{k-1},X_k]$ respectively), and replace the local times $L(x,k-1)$ by the numbers of up-crossings $\mathcal{E}^{k}_{x,+}$ (or $\mathcal{E}^{k}_{x,-}$ respectively). In fact, these two extended results are partial steps in the proof of Lemma 2.2 in \cite{KMP22}.   
		
		
		\section{Approximations of Local Drifts}\label{sec: approximations}
		In this section, we will prove the technical propositions in the proof of Theorem \ref{thm: main}. \TBD
		
		
		\subsection{Convergence of conditional Expectation}
		In view of the generalized P\'{o}lya urn process (associated to a site $y> x$), (\comment{maybe $ y>x>0 $ currently only one side is dealt}) on the event that $ L = \mathcal{E}^{(x,m)}_{y-1}$, we have \eqref{eq: conditional mean in GPU represenetation} 
		$$\rho^{(x,m)}_y = E[\mathcal{D}_{\tau_L^B}].$$ 
		On one hand, $M_{nt} \leq K\sqrt{n} $ for some $K>0$ with a high probability; on the other hand, Lemma \ref{lm: number of rarely visit sites} says that, up to $n^b$ sites, (where $\frac{\gamma \vee 0}{2}<b<\frac{1}{2}$,) every site $y$ between $X_k=x$ and $\mathcal{M}^{(x,m)} =S_{k}^X$ has $ \mathcal{E}^{(x,m)}_{y-1} \geq M  $ with a high probability. To show Proposition \ref{lm: approximation of means of local drift}, it suffices to show 
		\begin{equation}\label{eq: convergence of conditional expectation}
			\lim_{M\to\infty} E[\mathcal{D}_{\tau_M^B}] = \gamma , 
		\end{equation} for positive sites. A symmetric argument allows us to get the factor $sgn(y)$ for sites $y<0$ and $y=0$.
		
		\begin{lemma} \label{lm: convergence of mean of discrepancies}
			For the generalized P\'{o}lya urn process $(\mathcal{B}_{k},\mathcal{R}_{k})$ with weights $r(i)= w(2i)$, $b(i) = w(2i+1)$ for all $i\geq 0$, we have that
			$$
			\lim_{M\to\infty} E[\mathcal{D}_{\tau_M^B}] = \gamma. 
			$$
		\end{lemma} 
		\begin{remark}
			This lemma is an extension of Lemma~2.4 of \cite{KMP22} which only deals with the $p \in (\frac{1}{2}, 1]$ case. In their case, the local drift absolutely converges, which allows for a simpler argument. However, in the $p \in (0,\frac{1}{2}]$ case the local drift tends to infinity, which demands a more sophisticated argument.
		\end{remark}
		\begin{proof} 
			We start from \eqref{eq: gamma} and identity \eqref{eq: Toth's Identity 1}. For any $m \geq 10$,
			\begin{align}
				V_1(m) - U_1(m) =& \sum_{i=0}^{m-1} \frac{1}{w(2i+1)} -\sum_{i=0}^{m-1} \frac{1}{w(2i)} 
				\notag \\
				=& \sum_{i=0}^{m-1} \frac{1}{b(i)} -\sum_{i=0}^{m-1} \frac{1}{r(i)} 
				\notag \\
				=& 	\mathbb{E}\left[  \sum_{j=0}^{ \mu(m)-1 } \frac{1}{r(j)}   \right] - \sum_{i=0}^{m-1} \frac{1}{r(i)} = \mathbb{E}\left[  \sum_{j=0}^{ \mu(m)-1 } \frac{1}{r(j)}    - \sum_{i=0}^{m-1} \frac{1}{r(i)}\right]. \label{eq: difference}
			\end{align}
			From \eqref{eq: asymptotics of w}, we have that $0< \inf \frac{1}{r(j)} \leq \sup \frac{1}{r(j)} <\infty $, then $\mathbb{E}\left[\mu(m)\right]$ is bounded by
			$$\mathbb{E}\left[ \mu(m) \right] = \mathbb{E}\left[ \mu(m)\mathbb{1}_{\mu(m)\geq 1} \right] \leq  \frac{1}{\inf 1/w(j) }\mathbb{E}\left[  \sum_{j=0}^{ \mu(m)-1 } \frac{1}{r(j)}   \right] <\infty, $$ 
			so $ \mathbb{E}\left[ \mu(m) -m\right]  $ is bounded.
			The difference in \eqref{eq: difference} can be written as
			\begin{align} 
				\sum_{j=0}^{ \mu(m)-1 } \frac{1}{r(j)} - \sum_{i=0}^{m-1} \frac{1}{r(i)} =& \sum_{j=m}^{\mu(m)-1} \left(\frac{1}{r(j)} -\frac{1}{r(m)} \right) \cdot\mathbb{1}_{\{\mu(m)\geq m\}} 
				\label{eq: 1st term}
				\\	
				& - \sum_{j=\mu(m)}^{m-1} \left(\frac{1}{r(j)} -\frac{1}{r(m)} \right) \mathbb{1}_{\{\mu(m)< m\}} 
				\label{eq: 2nd term}
				\\
				& + \frac{\mu(m)-m}{ r(m) }. \label{eq: major term}
			\end{align} 
			Since $\mu(m)-m =  \mathcal{D}_{\tau^B_m}$, the last term \eqref{eq: major term} is exactly $\frac{1}{r(m)} \mathcal{D}_{\tau^B_m}$, which has an expectation $\frac{1}{r(m)} \mathbb{E}\left[\mathcal{D}_{\tau^B_m}\right].$ Both \eqref{eq: 1st term} and \eqref{eq: 2nd term} have finite expectations, which vanish as $m$ goes to infinity:
			
			Indeed, let $A> \frac{2}{c} \vee 1$, where $c$ is from Lemma \ref{lm: concentration inequality}. \eqref{eq: 1st term} is bounded by
			\begin{align}
				\sum_{j=m}^{\infty} \left(\abs{\frac{1}{r(j)} -\frac{1}{r(m)} } \cdot \mathbb{1}_{\{\mu(m)\geq j \}}\right) & \leq  \sum_{0\leq j-m \leq A \sqrt{m}\log m } \left(\abs{\frac{1}{r(j)} -\frac{1}{r(m)} } \cdot \mathbb{1}_{\{\mu(m)\geq j \}}\right) 
				\notag
				\\
				& +  \sum_{j-m > A\sqrt{m}\log m } \left(\abs{\frac{1}{r(j)} -\frac{1}{r(m)} } \cdot \mathbb{1}_{\{\mu(m)\geq j \}}\right)
				\notag
				\\
				&\leq  \sum_{0\leq j-m \leq A\sqrt{m}\log m } \abs{\frac{1}{r(j)} -\frac{1}{r(m)} }
				\label{low difference}
				\\
				& + 2\left(\sup_j \frac{1}{w(j)}\right) \cdot \sum_{j\geq m + A\sqrt{m}\log m } \mathbb{1}_{\{ \mu(m) > j \}}
				\label{large difference}
			\end{align}
			In view of \eqref{eq: asymptotics of w}, there is a constant $C'>0$ such that for any $m>100 $ and any $j$ with $\abs{j-m}\leq A \sqrt m \log m $, 
			$$ \abs{\frac{1}{r(j)} -\frac{1}{r(m)} } \leq C' A m^{-p-\frac{1}{2}} \log m, $$
			which implies that \eqref{low difference} is bounded by
			$$
			C' A^2 m^{-p} (\log m)^2.
			$$ On the other hand, Lemma \ref{lm: concentration inequality} implies that the expectation of \eqref{large difference} is bounded by
			\begin{align*}
				& 2\left(\sup_j \frac{1}{w(j)}\right) \sum_{j-m \geq A \sqrt m \log m  } P( \mathcal{D}_{\tau^B_m} \geq j-m )  
				\notag 
				\\
				\leq& 2\left(\sup_j \frac{1}{w(j)}\right) \sum_{l \geq A \sqrt m \log m } C \exp\left( - \frac{c  \cdot l^2}{l \vee m}   \right)
				\notag\\
				\leq& C'' \left( \exp (- cA^2 \cdot \log m ) + \exp(-cA \cdot \log m) \right), 
			\end{align*} for some $C''$ independent of $m$. Therefore, the expectation of \eqref{eq: 1st term} is bounded by
			\begin{equation}\label{boound}
				C' A^2 m^{-p} \log m + C''  \left( m ^{-cA^2} +  m^{-cA} \right). 
			\end{equation}
			One can treat \eqref{eq: 2nd term} similarly. 
			
			With our choice of $A >\frac{2}{c} \vee 1$, \eqref{eq: difference}, \eqref{eq: 1st term}, \eqref{eq: 2nd term}, \eqref{eq: major term}, and \eqref{boound}, we get that
			$$ \abs{ V_1(m)- U_1(m) -\frac{1}{r(m)}\mathbb{E}\left[ \mathcal{D}_{\tau^B_m} \right] }
			\leq 2C' A^2 m^{-p} \log m + 2C''  \left( m ^{-cA^2} +  m^{-cA} \right), 
			$$ which converges to $0$ as $m$ goes to infinity. We conclude that
			$$
			\lim_{m\to\infty}\mathbb{E}\left[ \mathcal{D}_{\tau^B_m} \right] = \gamma, 
			$$ from $\lim_{m\to\infty}\frac{1}{r(m)} =1$ and $ \lim_{m\to \infty} \left(V_1(m)-U_1(m) \right) = \gamma$.
		\end{proof}
		
		For the generalized P\'{o}lya urn process associated to a site $y<0$, we have $r(i) = w(2i+1)$, $b(i) =w(2i)$. The right hand side of \eqref{eq: difference} is the same as $U_1(m)-V_1(m)$, which converges to $-\gamma$ as $m$ goes to infinity. Then we get that  $$\lim_{m\to\infty}\mathbb{E}\left[ \mathcal{D}_{\tau^B_m} \right] = -\gamma.$$
		Similarly, for $y=0$, the associated generalized P\'{o}lya urn process has 
		$$\lim_{m\to\infty}\mathbb{E}\left[ \mathcal{D}_{\tau^B_m} \right] = 0.$$
		
		\TBD \textcolor{red}{We will need to define $\rho$ for negative sites.} We extend the definition of $\rho^{(x,m)}_y$ for sites $y< x$ by symmetry, 
		\[
			\rho^{(x,m)}_y := \mathbb{E}\left[  \Delta^{(x,m)}_y   \left\vert  \sigma\left( \mathcal{E}^{(x,m)}_{z, -}:  y<z\leq  x  \right. \right) \right].   
		\] 
		In terms of the generalized P\'{o}lya urn process associated to the site $y$,
		\begin{equation} \label{eq: extended definition}
			\rho^{(x,m)}_y = \mathbb{E}\left[\mathcal{D}_{\tau_L^R}\right], 
		\end{equation} where $L = \mathcal{E}^{(x,m)}_{y+1,-} = \mathcal{E}^{(x,m)}_{y,+}.$ With an argument similar to the proof of Lemma \ref{lm: convergence of mean of discrepancies}, we get that 
		\begin{equation}\label{eq: mean of discrepancies for left sites}
			\lim_{L\to \infty} \mathbb{E}\left[\mathcal{D}_{\tau_L^R}\right] =  sgn(y) \cdot \gamma = \lim_{L\to \infty} \mathbb{E}\left[\mathcal{D}_{\tau_L^B}\right].
		\end{equation}
		
		Now we show Proposition \ref{lm: approximation of means of local drift} and a slightly stronger result. 
		\begin{lemma}
			Let $w(.)$ be a positive monotone function on $\mathbb{N}_0$ satisfying \eqref{eq: asymptotics of w}. Then for any $0<p<1$, any $\epsilon>0$,
			$$
			\lim_{n\to\infty} P\left( \sup_{k\leq n t}  \abs{  	\sum_{y\in \left[X_{k}+1 ,S_{k}^X\right]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \geq  \epsilon \sqrt{n}     \right) =0.
			$$
			Furthermore,
		\[
	\lim_{n\to\infty} P\left( \sup_{k\leq n t}  \abs{  	\sum_{y\in \left[I_k^{X} ,S_{k}^X\right]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \geq  \epsilon \sqrt{n}     \right) =0.
	\]
\end{lemma}
\begin{proof} There are only three types of weight sequences for the generalized P\'{o}lya urn processes, see \eqref{eq: generalized weights}. Therefore, from Lemma \ref{lm: convergence of mean of discrepancies}, and \eqref{eq: mean of discrepancies for left sites}, there is a decreasing function $C(.)$ on $\mathbb{N}_0$ with $\lim_{L\to \infty}C(L) =0$ such that for any $y \in \mathbb{Z}$,
	\begin{equation}\label{eq: uniform convergence}
		\abs{\mathbb{E}\left[ \mathcal{D}_{\tau_L^R} \right] - \gamma \cdot sgn(y)}, \abs{\mathbb{E}\left[ \mathcal{D}_{\tau_L^B} \right] - \gamma \cdot sgn(y)} \leq C(L).
	\end{equation} One such function is $C(l) = \sup \left\{  \abs{\mathbb{E}\left[ \mathcal{D}_{\tau_m^R} \right] - \gamma \cdot sgn(y)} + \abs{\mathbb{E}\left[ \mathcal{D}_{\tau_m^B} \right] - \gamma \cdot sgn(y)} : m\geq l \right\}.     $  
	
	
	Let $t>0$, and $b \in [\frac{\gamma \vee 0 }{2},\frac{1}{2})$.  For any $n,K,M>0$, we consider three types of events, 
	$$A_{n,K}:=\left\{ \min\{-I_{nt}, M_{nt}\} \geq K \sqrt{n}  \right\}$$
	$$B_{n,M}:= \left\{  \sup_{k\leq n t} \sum_{ y\in (X_{k-1}, M_{k-1}]}  \mathbb{1}_{\{ \mathcal{E}^{k-1}_{y-1,+} \leq M  \}} >n^b  \right\},  $$
	and 
	$$B'_{n,M}:=  \left\{  \sup_{k\leq n t} \sum_{ y\in [I_{k-1}, X_{k-1})}  \mathbb{1}_{\{ \mathcal{E}^{k-1}_{y+1,-} \leq M  \}} >n^b  \right\}.$$
Clearly, $A_{n,K}$ is decreasing in $K$, and $B_{n,M}, B'_{n,M}$ are increasing in $M$. We claim that for $n$ large, the event 
$$
F_{n,\epsilon}:= \left\{ \sup_{k\leq n t}  \abs{  	\sum_{y\in [X_{k}+1 ,M_k]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \geq  \epsilon \sqrt{n}    \right \}$$ is contained in $A_{n,K} \cup B_{n,M} $ for some finite $K, M$ independent of $n$:   

	Indeed, depending on $(\mathcal{E}^{k-1}_{y-1,+} \leq M)$, terms  $\left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right)$ are bounded by $C(0)$ or $C(M)$. Therefore, the supremum is bounded by
	\begin{align*}
	 \sup_{k\leq n t}  \abs{  	\sum_{y\in [X_{k}+1 ,M_k]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \leq &  
	 C(0) \cdot \sup_{k\leq n t} \left\{   	\sum_{y\in [X_{k}+1 ,M_k]} \mathbb{1}_{ \{ \mathcal{E}^{k-1}_{y-1,+} \leq M \} } \right\}
	 \notag
	 \\
	 +& C(M) \cdot \sup_{k\leq n t} \left\{   	\sum_{y\in [X_{k}+1 ,M_k]} \mathbb{1}_{ \{ \mathcal{E}^{k-1}_{y-1,+} \geq M \} } \right\},
\end{align*} which is bounded on $A^c_{n,K} \cap B^c_{n,M}$ by
\begin{equation}\label{eq: an upper bound on good set}
	C(0)n^b  + C(M) \left(K \sqrt{n} -n^b\right).
\end{equation} As $n$ goes to infinity, \eqref{eq: an upper bound on good set} is smaller than $\epsilon \sqrt{n}$ for any $K>0$ and any $M$ with $C(M) < \frac{\epsilon}{2K}$ . 
For such pairs of $(K,M)$, $A^c_{n,K} \cap B^c_{n,M} \subset F^c_{n,\epsilon}$ when $n$ is large,  and 
$$
\limsup_{n\to \infty} P(F_{n,\epsilon}) \leq \limsup_{n\to \infty}  P(A_{n,K}) +  \limsup_{n\to \infty}  P(B_{n,M}).
$$ In view of Lemma \ref{lm: number of rarely visit sites} and the explanation after it, the second term $$\limsup_{n\to \infty}  P(B_{n,M})=0.$$  The first term $\limsup_{n\to \infty}  P(A_{n,K}) $ vanishes as $K$ goes to infinity, which is a consequence of Proposition 2.1 \cite{KMP22}, or Corollary 1A \cite{T96}.
 
The proof of 
$$  
\lim_{n\to\infty} P\left( \sup_{k\leq n t}  \abs{  	\sum_{y\in [I_k ,M_k]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \geq  \epsilon \sqrt{n}     \right) =0
$$ 
follows a similar argument. In particular, if we replace the range from $[X_k+1, M_k]$ by $[I_k,M_k]$, the event 
$$
\tilde{F}_{n,\epsilon}:= \left\{ \sup_{k\leq n t}  \abs{  	\sum_{y\in [X_{k}+1 ,M_{k}]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \geq  \epsilon \sqrt{n}    \right \}
$$ 
is contained in $A_{n,K} \cup B_{n,M} \cup  B'_{n,M},$ for any $K>0$, and $M$ with $C(M) < \frac{\epsilon}{4K}$.
\end{proof}

\eqref{eq: an upper bound on good set} can be used as a crude estimate for 	$\sum_{y\in [X_{k}+1 ,M_{k}]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right)$   on the "good events" $A^c_{n,K}\cap B^c_{n,M}$.

\subsection{Approximation of Local Drifts by Conditional Means}
In this subsection, we prove Proposition \ref{lm: approx local drift by conditional means}. This proof is similar to, and slightly more technical than the proof of (Lemma~4.2, \cite{KP16}). 

In \eqref{eq: control of martingale difference for local drift}, for a fixed $(x,m)$, the inner sum is a martingale. To control the sum, we restrict on a good event where this martingale has bounded increments. 

Consider some site $x > 0$ and let $m > 0$. \TBD. (enrich the argument to include discussion for the case $y < x$.) 
For integers $K>0$, $n > 0$ and $y > x$ we define the good event

\begin{align}
	G_{n,K}^{(x,m)}(y) :=  \qquad
		& \left\{ L(y,  \lambda_{x, m}  ) < K \sqrt{n} \right\} \cap \\
		& \bigcap_{i = 1}^{L(z, \lambda_{x, m}) } \left\{\left| \tau_i^{\mathcal{B}, y} - 2 i \right| < \sqrt{ i } \log^2 n \right\}  \cap \\
		& \bigcap_{i = 1}^{L(z, \lambda_{x, m}) } \left\{\left| \tau_{i+1}^{\mathcal{B},y} - \tau_i^{\mathcal{B},y} \right| < \log^2 n \right\}  
.\end{align}

Then  $\left\{G_{n, K}^{(x,m)}(y)\right\}_{y \ge x}$ is $\mathcal{H}^{(x,m)}$-adapted, that is, $G_{n, K}^{(x,m)}(y)\in \mathcal{H}_{y, +}^{(x,m)}$.

\begin{align}
	G_{n,K,t} :=  \qquad
		\label{eqn:good-event-1}
		& \left\{\sup _{k \le \left\lfloor nt  \right\rfloor} |X_k| < K \sqrt{n} \right\} \cap \\
		\label{eqn:good-event-2}
		& \left\{\sup_{y \in \mathbb{Z}} L(y, \left\lfloor nt  \right\rfloor) < K \sqrt{n} \right\} \cap \\
		\label{eqn:good-event-3}
		& \bigcap_{y = - K \sqrt{n} }^{K \sqrt{n}} 
		\bigcap_{i = 1}^{K \sqrt{n} } \left\{\left| \tau_i^{\mathcal{B}} - 2 i \right| < \sqrt{ i } \log^2 n \right\}  \cap \\
		\label{eqn:good-event-4}
		& \bigcap_{y = - K \sqrt{n} }^{K \sqrt{n}} 
		\bigcap_{i = 0}^{K \sqrt{n} } \left\{\left| \tau_{i+1}^{\mathcal{B}} - \tau_i^{\mathcal{B}} \right| < \log^2 n \right\}  
.\end{align}


Note that  if we denote \eqref{eqn:good-event-1} by $A_{n, K, t}$, then
	\begin{equation}
		\label{eqn:goodgood}
		G_{n, K, t} \subset  A_{n, K, t} \cap 
		\bigcap_{|x|, m < K \sqrt{n} } \bigcap_{y \in \mathbb{Z}} G_{n, K}^{(x,m)}
	.\end{equation} 

\begin{lemma}
	\label{lem:good-event}
	For any $t > 0$, The good event $G_{n,K,t}$ satisfies
	\[
		\lim_{K \to \infty } \limsup_{n \to \infty } 
		P(G_{n, K,t}) = 1
	.\] 
\end{lemma}



\begin{lemma}\label{lm:lipchitz-bound-on-good-event}
	For all  $y \ge x$, on $G_{n, K}^{(x,m)}(y)$, when $p \in (0,\frac{1}{2})$,  we have
	\begin{align*}
		\left| \Delta_y^{(x,m)} \right| &\le C_K n^{-\frac{1}{2}p + \frac{1}{4}} \log^4 n &\text{when }p \in \left(0,\frac{1}{2}\right)\\
		\left| \Delta_y^{(x,m)} \right| &\le C_K \log^5 n &\text{when }p = \frac{1}{2}
	.\end{align*}
	As a corollary, those bounds also apply to $\left| \Delta_y^{(x,m)} - \rho_y^{(x,m)} \right| $, up to a fixed multiplicative constant.
\end{lemma}

Now $\sum_y \Delta_y^{(x,m)} - \rho_y^{(x,m)}$ is a martingale, and on the event $G_{n, K, t}$ it should have bounded increments. We define a tempered version of the martingale that is Lipchitz:
\[
	\tilde \Delta_y^{(x,m)} := \Delta_y ^{(x,m)} \mathbf{1}\left( G_{n, K^2}^{(x,m)} (y)\right) \qquad
	\tilde \rho_y^{(x,m)} := \mathbb{E}\left[ \tilde\Delta_y^{(x,m)} \middle| \mathcal{H}_{y-1}^{(x,m)} \right]  
.\] 

Now we have the following decomposition of total drift at $\lambda_{x, m}$ :
\begin{align}
	&\sum_{y > x} \Delta_y^{(x,m)} - \gamma \\
	\label{eqn:tempered-difference-1}
	&= \sum_{y > x} \Delta_y^{(x,m)} - \tilde\Delta_y^{(x,m)} \\
	\label{eqn:tempered-difference-2}
	&+ \sum_{y > x} \tilde \Delta_{y}^{(x,m)} - \tilde\rho_y^{(x,m)} \\
	\label{eqn:tempered-difference-3}
	&+ \sum_{y > x} \tilde\rho_y^{(x,m)} - \rho_y^{(x,m)} \\
	\label{eqn:tempered-difference-4}
	&+ \sum_{y > x} \rho_y^{(x,m)} - \gamma
.\end{align}

On $G_{n, K, t}$, \eqref{eqn:tempered-difference-1} is zero. Since \eqref{eqn:tempered-difference-2} is a Lipchitz martingale, it can be bounded on $G_{n, K, t}$ using Azuma's inequality, and we will do later. Proposition~\ref{lm: approximation of means of local drift} has controlled \eqref{eqn:tempered-difference-4}. It remains to control \eqref{eqn:tempered-difference-3} on $G_{n, K, t}$. 
	\begin{align*}
		\sum_{y > x} \tilde\rho_y^{(x,m)} - \rho_y^{(x,m)}
		&= \sum_{y > x} \mathbb{E}\left[ \Delta_y^{(x,m)}\mathbf{1}\left( G_{n, K^2}^{(x,m)}(y) \right) - \Delta_{y}^{(x,m)} \middle| \mathcal{H}_{y-1}^{(x,m)}  \right]  \\
		&= \sum_{y > x} -\mathbb{E}\left[ \Delta_y^{(x,m)}\mathbf{1}\left( \left( G_{n, K^2}^{(x,m)}(y) \right) ^c \right) \middle| \mathcal{H}_{y-1}^{(x,m)}  \right]  \\
		\intertext{We have on $G_{n, K, t}$,}
		&= \sum_{y > x}^{\tau} -\mathbb{E}\left[ \Delta_y^{(x,m)}\mathbf{1}\left( \left( G_{n, K^2}^{(x,m)}(y) \right) ^c \right) \middle| \mathcal{H}_{y-1}^{(x,m)}  \right] \mathbf{1}\left(G_{n, K}^{(x,m)}(y-1)\right)
	.\end{align*}
Its absolute value is bounded by
\begin{align*}
	K \sqrt{n} \sqrt{
			\mathbb{E}\left[ \Delta_{x+1}^2 | \mathcal{E}_{x}^{(x,m)} = K \sqrt{n}  \right]
			P\left( \left( G^{(x,m)}_{n, K^2}(x+1) \right) ^{c} \middle| \mathcal{E}_{x}^{(x,m)} = K \sqrt{n}  \right) 
	}
.\end{align*}

\TBD


\begin{proof}[Proof of Proposition~\ref{lm: approx local drift by conditional means}]
	\edt{Edit needed, depends on previous lemma}
From Lemma~\ref{lm:lipchitz-bound-on-good-event}, we know that for some fixed $(x,m)$, the sum
\begin{equation}
	\label{eqn:good-martingale}
	\sum_{y > x}^{K \sqrt{n} } \left( \Delta_y^{(x,m)} \mathbf{1}\left( G_{n, K}^{(x,m)}(y) \right) - \tilde\rho_y^{(x,m)} \right)
\end{equation}
is a Lipchitz martingale adapted to $\mathcal{H}_{y,+}^{(x,m)}$.

We can apply Azuma's inequality to the sum in \eqref{eqn:good-martingale}. In the case $p < \frac{1}{2}$ we get for all $\varepsilon>0$,

\begin{align}
	P\left( \left| \sum_{y > x} (\Delta_y^{(x,m)} - \rho_y^{(x,m)}) \mathbf{1}\left(G_{n, K}^{(x,m)}(y)\right) \mathbf{1}\left(|y| < K\sqrt{n}\right) \right| > \varepsilon \sqrt{n}  \right) 
	&\le \exp\left( - \frac{\varepsilon^2 n}{2 K \sqrt{n} \left( C_{K} n^{-\frac{1}{2}p + \frac{1}{4}} \log^2 n  \right)^2 } \right) \notag \\
	&= \exp\left( - C_{K, \varepsilon} \, n^{p } \log^{-4} n \right)
   \label{eqn:azuma-drift-martingale}
   \intertext{
In the case $p = \frac{1}{2}$ the bound becomes
}
	\ldots
	&\le  \exp\left( - C_{K, \varepsilon} \, \sqrt{n}  \log^{-5} n \right)
.\end{align}
In both cases, the right hand side vanish when we first take $n $ to infinity, and then take $K$ to infinity.  

To bound $\sup_{k < nt} \left| \sum_{y > X_k} \Delta_y^{\left(X_k,L(X_k, k)\right)} - \rho_y^{\left(X_k,L(X_k, k)\right)} \right|$, we can do a supremum bound on all possible combinations of $(X_k, L(X_k, \left\lfloor nt  \right\rfloor)$. Thus, this amounts to bounding the supremum of $\sum_{y > x}\left| \Delta_y^{(x,m)} - \rho_y^{(x,m)} \right| $ over all $(x,m)$ pairs. Also note that, by \eqref{eqn:goodgood}, $G_{n, K}^{(x,m)}(y)$ is true for all $y \ge x$ on the event $G_{n, K, t}$.
\begin{align*}
	& P\left( \sup_{k < nt} \left| \sum_{y > X_k} 
	\Delta_y^{\left(X_k,L(X_k, k)\right)} - \rho_y^{\left(X_k,L(X_k, k)\right)}
	\right| > \varepsilon \sqrt{n}  \right) \\
	&\le P(G_{n, K, t}^c) + P\left( \sup _{|x|, m < K \sqrt{n} } \left| \sum_{y > x} \Delta_y^{(x,m)} - \rho_y^{(x,m)} \right|  > \varepsilon \sqrt{n} , \bigcap_{y > x} G_{n, K}^{(x,m)}(y) \right) \\
	&\le P(G_{n, K, t}^c) + K^2 n \sup _{|x|, m \le  K \sqrt{n} }
	P\left( \left| \sum_{y \ge X_k} \Delta_y^{(x,m)} - \rho_y^{(x,m)} \right|  > \varepsilon \sqrt{n} , \bigcap_{y > x} G_{n, K}^{(x,m)}(y) \right) \\
	&\le P(G_{n, K, t}^c) + K^2 n \left( \exp\left( - C_{K, \varepsilon} \, n^{p } \log^{-4} n \right) + \exp\left( - C_{K, \varepsilon} \, \sqrt{n}  \log^{-5} n \right) \right) 
.\end{align*}
In view of Lemma~\ref{lem:good-event}, the last line vanishes as we first take $n$ to infinity and then take $K$ to infinity.
\end{proof}

\vspace{2em}

\begin{proof}[Proof of Lemma~\ref{lem:good-event}]
	Fix $t>0$. We want to justify
	\[
		\lim_{K\to \infty } \liminf _{n \to \infty } P \left( G_{n, K, t}\right) = 1
	.\] 
	To do so, we treat each term \eqref{eqn:good-event-1}-\eqref{eqn:good-event-4} in the definition of $G_{n, K, t}$ separately.

	From process-level tightness of $\left( \frac{X_{\left\lfloor nt  \right\rfloor}}{\sqrt{n} } \right)_{t \ge 0} $ we know that probability of \eqref{eqn:good-event-1} goes to $1$ (uniformly in $n$) as $K $ goes to infinity. 

	The second event \eqref{eqn:good-event-2} provides uniform restriction to local time of all sites. Its probability is controlled by Lemma \TBD.

	The remaining two events \eqref{eqn:good-event-3} and \eqref{eqn:good-event-4} encode the asymptotic behavior of Polya's Urn processes at each site $y$. To establish the likelihood of \eqref{eqn:good-event-3}, note that the inner intersections over $i$ give rise to events that are i.i.d. over $y$. We may use concentration inequality for the Urn process (Lemma~\ref{lm: concentration inequality}) together with a supremum bound to control this event.
\begin{align*}
	1-P\left(\bigcap_{i = 1}^{K \sqrt{n} } \left\{\left| \tau_i^{\mathcal{B}} - 2 i \right| < \sqrt{ i } \log^2 n \right\}
\right) 
	&\le \sum_{y < \sqrt{n} }\sum_{i < K \sqrt{ n} } P\left( |\tau_i^{\mathcal{B}} - 2i| \ge \sqrt{i} \log^2 n \right) \\
	&\le CK \sqrt{n} \sum_{i < K \sqrt{ n} } \exp\left( - c \frac{i \log^4 n}{\sqrt{i}  \log^2 n \vee i} \right)  \\
	&\le CK \sqrt{n}  \sum_{i < K \sqrt{ n} }  
	\left( \exp\left( - c \sqrt{i}  \log^2 n \right)  + 
	\exp\left( - c \log^4 n \right) \right)
.\end{align*}
For fixed $K$, the last line goes to $0$ as $n$ goes to infinity. 

For \eqref{eqn:good-event-4}, we make a similar argument and note that, since the probability that the next ball drawn is blue is bounded below by some constant $q > 0$, $\tau_{i+1}^{\mathcal{B}} - \tau_{i}^{\mathcal{B}}$ is stochastically dominated by some geometric random variable $\text{Geo}(q)$. Hence for any specific $i$,

\[
	P\left(\left\{\left| \tau_{i+1}^{\mathcal{B}} - \tau_i^{\mathcal{B}} \right| \ge  \log^2 n \right\}\right) 
	\le C K^2 n \exp\left( - c \log^2 n \right) 
.\] 
This implies that the probability of event \eqref{eqn:good-event-4} goes to $1$ as $n$ goes to infinity, for any fixed $K$.

	We thus conclude that the probability of $G_{n, K, t}$ goes to $0$ as we first take $n$ to infinity and then take $K$ to infinity.
\end{proof}

\vspace{2em}

Finally, we verify the deterministic bound on martingale increments of \eqref{eq: control of martingale difference for local drift}, on $G_{n, K, t}$.

\begin{proof}[Proof of Lemma~\ref{lm:lipchitz-bound-on-good-event}]
	We verify this lemma for the case $y \ge  x > 0$. \TBD {\color{red} need to remark about other cases}.
	For notational simplicity, write $\alpha(i,j) := \frac{-w(2 i + 1) + w(2 j)}{w(2i + 1) + w(2 j}$. Then
\begin{equation*}
	\left| \Delta_y^{(x,m)} \right| 
	= 
	\left| 	\sum_{i = 0}^{\mathcal{E}_{y,+}^{(x,m)}} 
	\sum_{l = \tau_i^{\mathcal{B}}} ^{\tau_{i+1}^{\mathcal{B}}  -1}
	\alpha(\mathcal{B}_l, \mathcal{R}_l)
	\right| 
	\le 
	\sum_{i = 0}^{K \sqrt{n} } 
	\sum_{l = \tau_i^{\mathcal{B}}} ^{\tau_{i+1}^{\mathcal{B}}  -1}
	\left|
	\alpha(\mathcal{B}_l, \mathcal{R}_l)
	\right| 
\end{equation*}
and
\begin{align*}
	&\sum_{i = 0}^{K \sqrt{n} } 
	\sum_{l = \tau_i^{\mathcal{B}}} ^{\tau_{i+1}^{\mathcal{B}}  -1}
	|\alpha(\mathcal{B}_l, \mathcal{R}_l)|\\
	&\le C_w \sum_i \sum_l \left| \frac{1}{w(2 \mathcal{R}_l)} - \frac{1}{w(2 \mathcal{B}_l + 1)} \right|  \\
	&= C_w \sum_i \sum_l \left| \frac{1}{w(2 l - 2 i)} - \frac{1}{w(2i + 1)} \right|  \\
	&= C_w \sum_i \sum_{j = \tau_i^{\mathcal{B}} - i}^{\tau_{i+1}^{\mathcal{B}} - i - 1} \left| \frac{1}{w(2j)} - \frac{1}{w(2i + 1)} \right|  \\
	&= C_w \sum_i \sum_{j = \tau_i^{\mathcal{B}} - i}^{\tau_{i+1}^{\mathcal{B}} - i - 1} \left|  2^p B\left( \frac{1}{(2j)^p} - \frac{1}{(2i + 1)^p} \right)  + O\left( \frac{1}{i^{\kappa + 1}} \right) \right|  \\
	&\le C_w \sum_i \log^2 n \sup_{|j - i| \le 2 \sqrt{i}  \log^2 n} \left|  2^p B\left( \frac{1}{(2j)^p} - \frac{1}{(2i + 1)^p} \right)  + O\left( \frac{1}{i^{\kappa + 1}} \right) \right| \\
	&\le C_{w, p} \sum_i \log^2 n \left( 
		(4 \sqrt{ i } \log^2 n) (2 i - 2 \sqrt{ i } \log^2 n)^{- p - 1} + (2 i)^{- p - 1} + O(i^{- \kappa - 1})
	\right)  \\
	&\le C_{w, p} \sum_i \log^2 n \left( i ^{-p - \frac{1}{2}} \log^2 n +  i^{- \kappa - 1} \right)  \\
	&= C_{w, p} \sum_i i^{- p - \frac{1}{2}} \log^4 n + i^{- \kappa - 1 } \log^2 n \\
	&\stackrel{(p<\frac{1}{2})}{\le } C_{w, p} \left( (K \sqrt{ n} )^{-p+ \frac{1}{2}} \log^4 n + (K \sqrt{ n} )^{- \kappa } \log^2 n \right)  \\
	&= C_{w, p, K} n^{-\frac{1}{2}p + \frac{1}{4}  }  \log^4 n \qquad \text{assuming $\kappa$ small}
.\end{align*} 

{\color{red} Need some simplification and revision to make it more concise and cover all cases.}

\end{proof}



\subsection{Control of martingale Terms } 
\begin{proof}[Proof of Proposition~\ref{lm: control of martingale}]
It is already remarked earlier that we only need to obtain the control

\[
	 \lim_{N \to \infty } \frac{1}{N} \sum_{i = 0}^{N-1} \mathbb{E}\left[ X_{i+1} - X_i | \mathcal{F}_i \right]^2 = 0
.\] 


We can rewrite the sum into a sum of local drifts, and isolate the first $M$ visits:

\begin{align}
	&\sum_{i = 0}^{N-1} \mathbb{E}\left[ X_{i+1} - X_i | \mathcal{F}_i \right]^2\\
	&= \sum_{x \in \left[ I_N^X, S_N^X \right]} \sum_{i = 0}^{L_x(n) - 1} \mathbb{E}\left[ \mathcal{D}_{i+1}^{(x)} - \mathcal{D}_i^{(x)} | \mathcal{F}_{i}^{\mathcal{B}, \mathcal{R}} \right]^2  \\
	&\le  \sum_{x \in \left[ I_n^X, S_n^X \right]} \sum_{i = 0}^{L_x(N) - 1} \mathbb{E}\left[ \mathcal{D}_{i+1}^{(x)} - \mathcal{D}_i^{(x)} | \mathcal{F}_{i}^{\mathcal{B}, \mathcal{R}} \right]^2 \mathbf{1}\left( L_x(i) > M \right) + 
	\sum_{i = 0}^{N - 1} \mathbf{1}\left( L_{X_i}(i) \le  M \right) 
	\label{eqn:lem-martingale-1}
.\end{align}

We now restrict ourselves to the good event $G_{n, K, t}$ stated in \eqref{eqn:good-event-1}-\eqref{eqn:good-event-4}. Under $G$,
the inner sum in the first term is further controlled by
\begin{align*}
	&\sum_{i =0}^{ L_x(N) - 1} \mathbb{E}\left[ \mathcal{D}_{i+1}^{(x)} - \mathcal{D}_i^{(x)} | \mathcal{F}_{i}^{\mathcal{B}, \mathcal{R}} \right]^2 \mathbf{1}\left( L_{x}(i) > M \right) \\
	&\stackrel{(x > 0)}{\le} C_w \sum_{i = M}^{\mathcal{B}_N} \sum_{l = \tau_i^{\mathcal{B}}}^{\tau_{i+1}^{\mathcal{B}}-1} 
	\left| \frac{1}{w(2 \mathcal{R}_l)} - \frac{1}{w\left( 2 \mathcal{B}_l + 1 \right) } \right|^2 \\
	&\le C_{w, p} \sum_i i^{- 2 p - 1} \log^3 n \\
	\intertext{\color{red} \TBD need to fix the number of logs. Good event $G_{n, K, t}$ need to be handled more carefully.}
	&\le C_{w, p} \left[ M^{- 2 p} - \mathcal{B}_N^{- 2 p} \right] \log^3 N  \\
	&< C_{w, p} M^{-2 p} \log^3 N
	.
\end{align*}

Note that this bound is uniform in $x$. We remark that cases $(x=0)$ and $(x < 0)$ have the exactly same bounds. 

Now apply the bound $\sum_{i = 0}^{n-1} \mathbf{1}\left( L_{X_i}(i) \le M \right) \mathbf{1}(X_i = x) \le  M$ to the second term in \eqref{eqn:lem-martingale-1}, we have

\begin{multline}
	\sum_{x \in \left[ I_N^X, S_N^X \right]} \sum_{i = 0}^{L_x(N) - 1} \mathbb{E}\left[ \mathcal{D}_{i+1}^{(x)} - \mathcal{D}_i^{(x)} | \mathcal{F}_{i}^{\mathcal{B}, \mathcal{R}} \right]^2 \\
	< \sum_{x \in \left[ I_N^X, S_N^X \right]} (C_{w, p} M^{-2p} \log^3 N +M )
\end{multline}

Using process-level tightness, the summation only add a term of order $\sqrt{N}$, on a good event. Since we have a factor of $\frac{1}{N}$, and $M$ independent of $N$, we have the right hand side $\to 0$. More precisely,

\begin{align*}
	 &P\left( \left| \frac{1}{N} \sum_{i = 0}^{N-1} \mathbb{E}\left[ X_{i+1} - X_i | \mathcal{F}_i \right]^2  \right|  > \varepsilon \right)\\
	 &\le P\left( \left| \sum_{x \in \left[ I_N^X, S_N^X \right]} (C_{w, p} M^{-2p} \log^3 N +M ) \right| > \varepsilon  N \right) + P(G^c) \\
	 &\le 2 P\left( S_N^X \ge n^{3 / 4} \right) + P\left(  \left| \sum_{|x| \le N^{3 / 4}} (C_{w, p} M^{-2p} \log^3 n +M ) \right| > \varepsilon  n  \right) +P(G^c)  \\
	 &\le 2 P\left( S_N^X \ge n^{3 / 4} \right) + P\left(  C_{w, p, M} \,\, N^{3 / 4} \log^3 N > \varepsilon  N  \right) + P(G^c)
.\end{align*}
The second term vanishes trivially; the first term vanishes by process-level tightness of $S_N^X / \sqrt{N} $. The last term goes to zero as shown in the previous section.
\end{proof}

\printbibliography
\end{document}

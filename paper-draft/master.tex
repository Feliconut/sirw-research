%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Title and author(s)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Scaling Limit of AF-SIRW for $p\leq\frac{1}{2}$.}
\author{ 
	Xiaoyu Liu
	\and
	Zhe Wang}

\date{Aug 9th, 2023}


% This is the definition of the type of document
\documentclass[twoside,12pt,a4paper]{article}
%\documentclass{article}

\usepackage[english]{babel}
\usepackage[margin=1.0 in]{geometry}
\usepackage[latin1]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fourier}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage[inline]{enumitem}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{remark}{Remark}
\newtheorem{scolium}{Scolium} [section]  
\newtheorem{definition}{Definition}[section]
\numberwithin{equation}{section}

%% This defines the "proo" environment, which is the same as proof, but
\newenvironment{proof}{{\sc Proof}:}{~\hfill $\square$}
\newenvironment{AMS}{}{}
\newenvironment{keywords}{}{}

%% Local macros
\newcommand{\abs}[1]{\left\vert #1 \right\vert}
\newcommand\TBD{\textcolor{red}{TBD.}}






\begin{document}
		\maketitle

\begin{abstract}
	This document is an outline of the article for the Scaling limit of SIRW. We generalize the functional CLT in \cite{KMP22} for the asymptotically free self-interacting random walk (AF-SIRW) in the case $0<p \leq \frac{1}{2}$. The approach is to carefully approximate the local drifts of the random walk via the study of the directed edge local times, which are described by branching-like processes and generalized Ray-knight Theorems. Xiaoyu Liu and Zhe Wang are working on this project. 
	\TBD
\end{abstract}

\section{Introduction}
The current project is a generalization of the result in \cite{KMP22}. 
\TBD 


\subsection{Model Description} 
We consider a discrete time nearest neighbor self-interaction random walk $(X_i)_{i\geq 0}$ on $\mathbb{Z}$ starting from site $x=0$ with transition probabilities depending on its edge local times and a (deterministic) weight function $w$ on $\mathbb{N}\cup \{0\}$. More precisely, let $w(.)$ be a monotone positive function on $\mathbb{N}_0$ satisfying

\begin{equation}\label{eq: asymptotics of w}
	\frac{1}{w(n)} = 1+\frac{2^p B}{n^p} + O\left(\frac{1}{n^{1+\mathcal{\chi}}}\right), \quad \mbox{as $n\to \infty$}, 	
\end{equation} 
The SIRW $(X_i)_{i\geq 0}$ has following dynamics, 
$X_0 = 0$, and for any $n\geq 0$
\begin{equation}\label{dynamic}
	\mathbb{P}\left( X_{n+1} =  X_n +1 | X_0,X_1,\dots,X_n   \right) =  \frac{  w(r(X_i,i) )}{ w(l(X_i,i))  + w(r(X_i,i))   },
\end{equation}
where $l(x,i)$ and $r(x,i)$ are the local times by time $i$ for the undirected edges $(x,x-1)$ and $(x,x+1)$
$$ l(x,i) = \sum_{j=0}^{i-1} \mathbb{1}_{ \{  (X_j, X_{j+1}) =  (x,x-1) \} }, \mbox{ and }  r(x,i) = \sum_{j=0}^{i-1} \mathbb{1}_{ \{  (X_j, X_{j+1}) =  (x,x+1) \} }.        $$

In our case, the weight function $w(.)$ is deterministic and identical for each site $x\in \mathbb{Z}$. \TBD (Add backgrounds on the model assumptions, especially why we use deterministic monotone function $w$, and why this is called asymptotically free case. )

\subsection{Main Result}
\begin{definition}
	Let $\theta_+, \theta_- <1$. A BMPE $W^{\theta_+, \theta_-} = \left(W^{\theta_+, \theta_-}_t, t\geq 0\right)$ with parameter  $(\theta_+, \theta_-)$ is the pathwise unique solution of the equation
	$$
	W_t = B_t + \theta_+ \cdot \sup_{s\leq t} W_s  + \theta_- \cdot \inf_{s\leq t} W_s,   \quad W_0 = 0.
	$$
\end{definition}
When the weight function $w(.)$ is monotone and satisfying \eqref{eq: asymptotics of w}, for any $p\in (0,1]$
\begin{equation}\label{eq: gamma}
	\gamma:= \lim_{n\to \infty}\left( V_1(n) - U_1(n) \right) =\lim_{n\to \infty} \left( \sum_{j=0}^{n-1} \frac{1}{ w(2j+1)}-  \sum_{j=0}^{n-1}  \frac{1}{w(2j)} \right) 
\end{equation}
is well defined and $\gamma<1$. It is shown, in Theorem 1.2 \cite{KMP22}, that when $p\in (\frac{1}{2},1]$, and $\mathcal{\chi} >0 $, the rescaled process
$$
 \left(  \frac{X_{\lfloor nt \rfloor }}{\sqrt{n}}  \right)_{t\geq 0} \Longrightarrow \left( W^{\gamma,\gamma}_{t}\right)_{t\geq 0},
$$ as $n$ goes to infinity in the standard Skorohod topology $D([0,\infty) ).$

Our main result is the functional limit theorem in the case when $p\in (0,\frac{1}{2})$.
\begin{theorem}\label{thm: main}
 Let $w(.)$ be monotone and satisfy \eqref{eq: asymptotics of w} for $p\in (0,\frac{1}{2}]$, and $\mathcal{\chi} >0 $. Consider the SIRW $(X_n)_{n\geq 0}$ defined in \eqref{dynamic} with $X_0 =0$. Then the rescaled process
	 $$
	 \left(  \frac{X_{\lfloor nt \rfloor }}{\sqrt{n}}  \right)_{t\geq 0} \Longrightarrow \left( W^{\gamma,\gamma}_{t}\right)_{t\geq 0},
	 $$ as $n$ goes to infinity in the standard Skorohod topology $D([0,\infty) ).$
\end{theorem}
The proof of Theorem \ref{thm: main} follows a strategy similar to that Theorem 1.2 \cite{KMP22}, which relies on martingale methods, analysis of branching-like processes, and generalized Ray-Knight Theorems. This approach is classical in random walk in random environments \TBD, and it has also been used previously in other self-interacting random walk in dimension one, such as \TBD. The novelty of the current article is an approximation of accumulated local drifts $\Delta_y^{(x,m)}$, defined in section \ref{sec: proof of main} below on certain good events, which occurs with a high probability. On these good events, the accumulated local drifts are well approximated by their conditional means $\rho_{y}^{(x,m)}$ given certain edge local times $\mathcal{E}_y$, while their conditional means $\rho_{y}^{(x,m)}$ are close to the $\gamma \cdot sgn(y)$ when  $\mathcal{E}_y$ is large. \TBD These good events are closely related to the generalized P\'{o}lya  urn processes associated to sites, and we estimtate their probabilities by studying branching-like processes, which are derived from the original SIRW $(X_n)_{n\geq 0}$ via the generalized Ray-Knight Theorem. At this point, we also point out that the approximation of accumulated local drifts $\Delta_y^{(x,m)}$ is different from those in \cite{KMP22} since in the case when $p\in(\frac{1}{2},1]$, the conditional mean $\rho_{y}^{(x,m)}$ converges absolutely, which does not hold in the case when $p\leq \frac{1}{2}$. We need to construct certain good events on which the errors $\Delta_y^{(x,m)}- \rho_y^{(x,m)}$ are well-behaved.

\subsection{Organization of Paper}
In section \ref{sec: proof of main}, we prove Theorem \ref{thm: main} in several technical steps, each of which is stated as an individual proposition. In particular, the proofs of three technical results are postponed to section \ref{sec: approximations} because they involve analysis of auxiliary processes. In section \ref{sec: generalized Polya Urn, BLP}, we describe these auxiliary processes, generalized P\'{o}lya urn processes and branching-Like processes, discuss their connections to the SIRW $(X_n)$, and show some preliminary results related to technical results. In section \ref{sec: approximations}, we show the approximations of local drifts on good events as well as estimating their probabilities. 

\subsection{Notation}
We end this section by introducing some notations related to SIRW $(X_n)_{n\geq 0}$. To avoid confusion, the notations related to generalized P\'{o}lya urn, and branching-like processes are in section \ref{sec: generalized Polya Urn, BLP}.

For an SIRW $(X_n)_{n\geq 0}$,  let $(\mathcal{F}^X_n)_n$ be its natural filtration $\mathcal{F}^X_n = \sigma\left(X_i: i\leq n \right).$ We denote by $\mathbb{P}$ and $\mathbb{E}$ its probability and corresponding expectation. The following (random) quantities related to the SIRW $(X_n)_n$ are important in our analysis:
\begin{enumerate}
	\item the local time of site $x$ by time $j$, $L(x,j):= \sum_{i=0}^j \mathbb{1}_{\{X_i=x\} }$;
	
	\item the local time of directed bond $(x,x+1)$ by time $j$,
	$ \mathcal{E}^j_x = \sum_{i=0}^{j-1} \mathbb{1}_{\{X_i=x, X_{i+1} =x+1 \} } $; the local time of directed bond $(x,x+1)$ by time $j$ is $ D^j_x = \sum_{i=0}^{j-1} \mathbb{1}_{\{X_i=x, X_{i+1} =x-1 \} }; $
	
	
	\item the $m$-th visit time to site $x$, $\lambda_{x,m} = \inf\{t \geq 0: L(x,t) = m\}$;
	
	\item the running maximum of the process at time $n$, $M_n= \sup\{y: L(y,n)\geq 1 \} $; the minimum of the process at time $n$, $I_n= \inf\{y: L(y,n)\geq 1 \} $;
	
	\item the accumulated local drifts at site $x$ by time $k$, $$\delta_{x,m}:= \sum_{i=0}^\infty E[X_{i+1}-X_i\vert \mathcal{F}_{i}^X] \mathbb{1}_{\{X_i=x, L(x,i)\leq m\}};$$
	and the accumulated local drifts at site $x$ by time $\lambda_{x,m}$, 
	\begin{equation}\label{eq: accumulated local drift}
	\Delta_y^{x,m}:= \sum_{i=0}^{\lambda_{x,m}} E[X_{i+1}-X_i\vert \mathcal{F}_{i}^X] \mathbb{1}_{\{X_i=y\}}.
	\end{equation}
\end{enumerate}

\section{Proof of Theorem \ref{thm: main}}\label{sec: proof of main}
The proof of Theorem \ref{thm: main} follows a classical strategy in obtaining functional limit theorems. We will divide the proof in four main steps. We first decompose the random walk into
$X_n = M_n+ \Gamma_n, $ where
$$ 
\Gamma_0 = 0, \quad \Gamma_n = \sum_{i=0}^{n-1} \mathbb{E}\left[ X_{i+1}-X_i | \mathcal{F}_i^X \right]
$$ 

\textbf{Step 1: Control of martingale term.}
The choice of $\Gamma_n$ allows us to get a martingale $M_n$ with respect to $\mathcal{F}_i^X.$ Then the rescaled process $\frac{M_n}{\sqrt{n}}$ converges to a standard brownian motion in distribution if we have the control of martingales 
\begin{equation}\label{eq: QV term}
\lim_{n\to \infty}\frac{1}{n} \sum_{i=0}^{n-1}\mathbb{E}\left[ (M_{i+1}- M_{i})^2 |\mathcal{F}_i^X \right] =1,  \mbox{ in probability}.
\end{equation}
Since $\abs{X_{n+1}-X_n}=1$,  \eqref{eq: QV term} is implied by the estimate
\begin{proposition} \label{lm: control of martingale} Let $p\in (0,\frac{1}{2}]$. Then, for any $\epsilon >0$
	\begin{equation}\label{eq:  term}
	\lim_{n \to \infty }\mathbb{P}\left(\frac{1}{n} \sum_{i = 0}^{n-1} \mathbb{E}\left[ X_{i+1} - X_i | \mathcal{F}_i \right]^2 > \epsilon \right) =0. 
\end{equation}
\end{proposition}
The proof of Proposition \ref{lm: control of martingale} is in section \ref{sec: approximations}.

\textbf{Step 2: Control of accumulated drift.} This is the major technical step of our article. We want to approximate the accumulated drift $\Gamma_n$ by a fixed linear combination of the distances between $X_n$ and its running maximum $S_n$ and minimum $I_n$:
\begin{proposition}\label{lm: control of acc drift}
	 Let $p\in (0,\frac{1}{2}]$. Then, for any $t>0$ and any $\epsilon >0$
	\begin{equation}\label{eq: control of acc drift}
		\lim_{n \to \infty }\mathbb{P}\left(\sup_{k\leq nt} \abs{\Gamma_k - \gamma \left(M_k + I_k \right)   } > \epsilon \sqrt{n}  \right) =0. 
	\end{equation}
\end{proposition}
To prove Proposition \ref{lm: control of acc drift}, we assume that $X_n=x \geq 0$, and $L(x,n)=m$. We decompose the accumulated drift as $\Gamma_n = 	\Gamma_n^+ +	\Gamma_n^0 + \Gamma_n^-$ 
\begin{align}
	\Gamma_n^+ &= \sum_{y > X_n} \sum_{i = 0}^{n-1} \mathbb{E}\left[ X_{i + 1} - X_i | \mathcal{F}_i^X \right] \mathbf{1}(X_i = y)\\
	\Gamma_n^0 &= \sum_{i = 0}^{n-1} \mathbb{E}\left[ X_{i + 1} - X_i | \mathcal{F}_i^X \right] \mathbf{1}(X_i = y) \\
	\Gamma_n^- &= \sum_{y < X_n} \sum_{i = 0}^{n-1} \mathbb{E}\left[ X_{i + 1} - X_i | \mathcal{F}_i^X \right] \mathbf{1}(X_i = y)
.\end{align} 
The contribution from $\Gamma_n^0$ is negligible (\TBD). We only need to consider the term $\Gamma_n^+$ while the contribution from $\Gamma_n^-$ \textcolor{red}{almost follows a symmetric argument}. (\TBD) 

In view of \eqref{eq: accumulated local drift}, $\Gamma_n^+$ is the sum 
$$
\Gamma_n^+ = \sum_{y>x} \Delta_y^{(x,m)}.
$$ which has $(M_n - X_n)$ terms. If we can further approximate terms $\Delta_y^{(x,m)}$ by $\gamma$ without generating a total error of size $\epsilon \sqrt{n}$, $\Gamma_n^+$ is approximately 
$$   
 \gamma (M_n - X_n).
$$
More precisely,for any $t>0$ and any $\epsilon >0$
\begin{equation}\label{eq: control of acc drift + }
	\lim_{n \to \infty }\mathbb{P}\left(\sup_{k\leq nt} \abs{\Gamma^+_k - \gamma \left(M_k - X_k \right)   }\cdot \mathbb{1}_{\{X_k\geq 0 \}} > \epsilon \sqrt{n}  \right) =0. 
\end{equation}
To this end, we consider the filtration $\left(\mathcal{G}_{y}^{(x,m)}\right)_{y\geq x}$, where $ \mathcal{G}_{y}^{(x,m)} = \sigma\left( \mathcal{E}^{n}_y : y \geq x \right)$, and further approximate $\Delta_y^{(x,m)}$ by its conditional mean with respect to $\mathcal{G}_{y-1}^{(x,m)}$,
\begin{equation}\label{eq: conditional mean}
	\rho_{y}^{(x,m)}= \mathbb{E}\left[\Delta_y^{(x,m)} | \mathcal{G}_{y-1}^{(x,m)}\right].
\end{equation}
Then \eqref{eq: control of acc drift + } follows from the following two propositions:
\begin{proposition}\label{lm: approximation of means of local drift}
		Let $p\in (0,\frac{1}{2}]$. Then, for any $t>0$ and any $\epsilon >0$
		\begin{equation}\label{eq: control of expected local drift}
			\lim_{n \to \infty }\mathbb{P}\left(\sup_{k\leq nt} \abs{\sum_{y> X_k} \left( \rho_{y}^{(x,m)} - \gamma  \right)   }\cdot\mathbb{1}_{\{X_k\geq 0\}} > \epsilon \sqrt{n}  \right) =0. 
		\end{equation}
\end{proposition}
and
\begin{proposition}\label{lm: approx local drift by conditional means}
	Let $p\in (0,\frac{1}{2}]$. Then, for any $t>0$ and any $\epsilon >0$
	\begin{equation}\label{eq: control of martingale difference for local drift}
		\lim_{n \to \infty }\mathbb{P}\left(\sup_{k\leq nt} \abs{\sum_{y> X_k} \left(\Delta_{y}^{(x,m)}- \rho_{y}^{(x,m)} \right)   } \cdot\mathbb{1}_{\{X_k\geq 0\}} > \epsilon \sqrt{n}  \right) =0. 
	\end{equation}
\end{proposition}
To prove both Propositions \ref{lm: approximation of means of local drift} and \ref{lm: approx local drift by conditional means}, we will introduce auxiliary processes associated to the SIRW $(X_n)_{n\geq 0}$: the generalized P\'{o}lya Urn process, and branching-like processes. Both processes are natural projections of $(X_n)_{n\geq 0}$ on certain space-time points, and they aid our approximations of $\Delta_{y}^{(x,m)}$ on certain good events. The good events in the proof of Proposition \ref{lm: approximation of means of local drift} are easier because events like Lemma \ref{lm: number of rarely visit sites} below will almost do the job, and estimates of their probabilities are essentially known from \cite{KMP22}. However, the good events for Proposition \ref{lm: approx local drift by conditional means} requires a new construction, and estimating the error requires a new proof, which is the major novelty of this article. We will postpone the proofs of Propositions \ref{lm: approximation of means of local drift} and \ref{lm: approx local drift by conditional means} and constructions of good events to section \ref{sec: approximations} after introducing the generalized P\'{o}lya Urn process, and branching-like processes in section \ref{sec: generalized Polya Urn, BLP}.

\TBD With some slight efforts, we extend \eqref{eq: control of acc drift + } for $\Gamma_k^+$ to $\Gamma_k^-$, and obtain the full Proposition \ref{lm: control of acc drift}.


\textbf{Step 3: Tightness.} \TBD This follows from (\cite{KMP22}, Proposition~2.1).

\textbf{Step 4: Convergence to BMPE.} \TBD The proof follows word by word.

\section{Generalized P\'{o}lya Urn, Branching-Like Processes}\label{sec: generalized Polya Urn, BLP}
In this section, we describe two auxiliary processes, generalized P\'{o}lya urn processes and branching-like processes, and recall some of their properties needed in the proofs of Propositions \ref{lm: control of martingale}, \ref{lm: approximation of means of local drift}, and \ref{lm: approx local drift by conditional means}. Both processes can be obtained from the SIRW $(X_n)_{n\geq 0}$ at stopping times. We start with the generaized P\'{o}lya urn processes. 
\subsection{Generalized P\'{o}lya  Urn}
Given a (recurrent) random walk $(X_n)_{n\geq 0}$, and a fixed site $y\in \mathbb{Z}$, we can obtain a Markov process by considering only the up-crossings and down-crossings of $X_n$ from site $y$. More precisely, let $\lambda_k = \lambda_{y,k}$ be the event times that $X_n$ jump from site $y$ (\TBD{we can adjust the definition for local time}):
$$
\lambda_0 :=  \lambda_{y,0} =\inf\{ t> \lambda_{k}: X_t = y \} , \quad \lambda_{k+1} := \lambda_{y,k+1} = \inf\{ t> \lambda_{k}: X_t = y \}
$$
then the sequence 
$$
(\mathcal{B}_k,\mathcal{R}_k )_k:=\left(\mathcal{E}^{\lambda_{y,k}}_y, \mathcal{D}^{\lambda_{y,k}}_y\right)_{k\geq 0}  
$$ is a Markov process with an initial value $(0,0)$. We call this Markov process $(\mathcal{B}_k,\mathcal{R}_k )_k$ a generalized P\'{o}lya urn process associated to site $y$. Due to the initial value of the underlying random walk $X_0=0$, the local times of "undirected edges" $\{x,x+1\}$ and $\{x,x-1\}$, 
$$l(x,i)+r(x-1,i), \quad  r(x,i)+l(x+1,i)  $$ are different at the first visit time $\lambda_{y,0}$ for values $y>0$, $y<0$ or $y=0$. Therefore, we get three types of transition probabilities
for the generalized P\'{o}lya urn process depending on $y>0$, $y<0$ or $y=0$:
\begin{align*}\label{eq: transition prob for GPU}
	\mathbb{P} \left((\mathcal{B}_{k+1},\mathcal{R}_{k+1})=  (i+1,j) \vert (\mathcal{B}_{k},\mathcal{R}_{k}) =(i,j)  \right) &= \frac{b(i)}{b(i)+r(j)}, \mbox{ and}  \\
	\mathbb{P} \left((\mathcal{B}_{k+1},\mathcal{R}_{k+1})=  (i+1,j) \vert (\mathcal{B}_{k},\mathcal{R}_{k}) =(i,j)  \right) &= \frac{r(j)}{b(i)+r(j)},
\end{align*} where the generalized weights $(b(k),r(k))_{k\geq 0}$ depend on $w(.)$ and $y$,  
\begin{equation}\label{eq: generalized weights}
	(b(k), r(k)) = \begin{cases}
		(w(2k+1), w(2k)) &,  \text{ if }  y>0 \\
		 (w(2k), w(2k+1)) &,  \text{ if }  y<0 \\  
		 (w(2k), w(2k)) &,  \text{ if }  y=0 \\ 
	\end{cases}.
\end{equation}
For a generalized P\'{o}lya urn process $(\mathcal{B}_k,\mathcal{R}_k )_k$ associated to the site $y$, it is convenient to consider the event times of left/right jumps $\tau^B_j$ and $ \tau^R_j $,	
$$ \tau^B_j = \inf\{ j\geq 0: \mathcal{B}_{k} =j   \}  \mbox{, and } \tau^R_j = \inf\{ k\geq 0: \mathcal{R}_{k} =j   \},
$$ and the signed difference $\mathcal{D}_{k} $,
\begin{equation}\label{eq:signed difference}
\mathcal{D}_k  =\mathcal{R}_k -\mathcal{B}_k.  
\end{equation} 

\TBD{(Shall we give a proof of these two equations?) With the generalized P\'{o}lya urn process $(\mathcal{B}_k,\mathcal{R}_k )_k$ associated to the site $y$, we can study approximations of $\Delta_{y}^{(x,m)}$ and $\rho_{y}^{(x,m)}$ defined in \eqref{eq: accumulated local drift} and \eqref{eq: conditional mean}. As we will see later from the branching-like processes, on the event that $ L(y,\lambda_{x,m}) = k$,  the (random) quantity $\Delta_{y}^{(x,m)}$ is the same as
$$
\Delta_{y}^{(x,m)} = \mathbb{E}\left[ \mathcal{D}_k \right].
$$ 
Similarly, from \eqref{eq: conditional mean} when $y> x >0$, on the event that $ \mathcal{E}^{(x,m)}_{y-1} = L $, we can rewrite the (random) quantity $ \rho_{y}^{(x,m)} $ as
\begin{equation}\label{eq: conditional mean in GPU represenetation}
	\rho_{y}^{(x,m)} = \mathbb{E}\left[\Delta_{y}^{(x,m)} \vert \mathcal{G}_{y-1}^{(x,m)}\right] = \mathbb{E}\left[ \mathcal{D}_{\tau^B_L}  \right]\mathbb{1}_{\{  \mathcal{E}^{(x,m)}_{y-1} = L \}},
\end{equation} which is $\mathcal{G}_{y-1}^{(x,m)}$- measurable since $\mathcal{E}^{(x,m)}_{y-1} $ is $\mathcal{G}_{y-1}^{(x,m)}$- measurable. Expressions like \eqref{eq: conditional mean in GPU represenetation} help us simplify the analysis of $\Delta_{y}^{(x,m)}$ and $\rho_{y}^{(x,m)}$, as will be shown section \ref{sec: approximations}. To estimate events like $ L(y,\lambda_{x,m}) = k$ and $\mathcal{E}^{(x,m)}_{y-1} = L $, the branching-like processes play a key role. 


\subsection{Branching-Like Processes}
Unlike the generalized P\'{o}lya urn process $(\mathcal{B}_k,\mathcal{R}_k )_k$ which is a Markov process derived from $X_n$ associated to a single site $y$ in $\mathbb{Z}$, the branching-like processes is a Markov process spatially, and it describes the local times of directed edges of $(X_n)_{n\geq 0}$ at a fixed stopping time. More precisely, for an SIRW $(X_n)_{n\geq 0}$ with weight $w(.)$, at any fixed time $n \geq 0$, assuming that $X_n = x \geq 0 $, the local times $\tilde{\zeta}= (\mathcal{E}^{n}_{x+k} )_{k\geq 0}$, and ${\zeta}= (\mathcal{D}^{n}_{x-k} )_{k\leq 0}$ are two Markov processes on $\mathbb{N}\cup\{0\}$, whose transition probabilities are related to the generalized weights in \eqref{eq: generalized weights}. The transition probabilities can be derived from a correspondence from one dimensional random walk and a branching process. Formally, we can project a random walk on $\mathbb{Z}$ (up to any finite time) on a tree by viewing each $X_n= k$ as an agent of the $k$-th generation. Each right move from $X_n=k$ to $X_{n+1}=k+1$ gives a birth to a descendant in the $k+1$-th generation, and each left move traces the ancestor of $X_n=k$ in the $k-1$-th generation. It is possible that an agent $A$ corresponding to $X_n=k$ in the $k$-th generation has more than one $n$ with $X_n=k$, and it is visited exactly $L+1$ times if it has exactly $L$ descendants in the $k+1$-th generation and $X_T $ is not in the sub-tree with the root $A$. 
Then, since $X_0=0$ and $X_n =x \geq 0$, the difference $\mathcal{E}^n_y -\mathcal{D}^n_{y+1} $ depends on the relative position of $y$ with respect to $x$ and $0$ 
\begin{equation}\label{eq: edge values}
	\mathcal{E}^n_y =\begin{cases}
		\mathcal{D}^n_{y+1} &\mbox{, for all $y\geq x$}\\
		\mathcal{D}^n_{y+1} +1 &,\mbox{ for all $0 \leq y\leq x$}
		\\
		\mathcal{D}^n_{y+1} &,\mbox{ for all $ y< 0 $}
	\end{cases}
\end{equation}
The transition probabilities for $\bar{\zeta}$ is more direct because upcrossings in $X_n$ corresponds to giving births to new descendants in the branching process, while $\zeta$ is slightly more complicated and $\zeta$ has a transition probabilities depending on values of $y,0,x$: 
\begin{enumerate}
	\item $\tilde{\zeta}$ is a homogeneous Markov chain defined on  $\mathbb{N}\cup\{0\}$ with $i,j\geq 0$
	\begin{equation}\label{eq: transition prob on positive}
		\mathbb{P}\left(\tilde{\zeta}_{k+1}=j \vert \tilde{\zeta}_k =i  \right) = 
		\mathbb{P}\left( \mathcal{R}_{\tau_i^B} = j \right), 
	\end{equation} where $(\mathcal{B}_k,\mathcal{R}_k )_k$ has generalized weights $(b(k), r(k))$ corresponds to the case when $y>0$.
	$$
	(b(k), r(k)) = (w(2k+1), w(2k)).
	$$
	
	\item  $\zeta$ is an inhomogeneous Markov chain defined on  $\mathbb{N}\cup\{0\}$ whose transition probabilities depends on the values of $x-k<0$, $x-k = 0$ , and $x-k >0$, for $i,j\geq 0$
	\TBD(needs to double check the formula)
	\begin{equation}\label{eq: transition prob on positive }
		\mathbb{P}\left(\tilde{\zeta}_{k+1}=j \vert \tilde{\zeta}_k =i  \right) = 
		\begin{cases}
		\mathbb{P}\left( \mathcal{B}_{\tau_{i+1}^R} = j \right) ,& \mbox{ if $0 \leq k <  x-1$ }
		\\
		\mathbb{P}\left( \mathcal{B}_{\tau_{i+1}^R} = j \right) ,& \mbox{ if  $k =  x-1$ }
		\\
		\mathbb{P}\left( \mathcal{R}_{\tau_i^B} = j \right) ,& \mbox{ if $k \geq x$ }
		\end{cases}
	\end{equation} where the generalized weights $(b(k), r(k))$ corresponds to the case when $y>0$, $y=0$ and $y<0$ from \eqref{eq: generalized weights}
	$$
	(b(k), r(k)) = \begin{cases}
	(w(2k+1), w(2k)) &,  \text{ if }  y>0 \\
	(w(2k), w(2k+1)) &,  \text{ if }  y<0 \\  
	(w(2k), w(2k)) &,  \text{ if }  y=0 \\ 
\end{cases}.
	$$	 
 \end{enumerate}
In the case when $X_n = z\leq 0$, we can exchange the roles of $\zeta$ and $\bar{\zeta}$ and consider the local times of downcrossings/upcrossings away from site $x\leq 0$,  $\bar{\zeta'}= (\mathcal{D}^{n}_{x-k} )_{k\leq 0}$ and $\zeta'= (\mathcal{E}^{n}_{x+k} )_{k\geq 0}$. Due to symmetry, the new processes $\bar{\zeta'}$ and $\zeta'$ for $x\leq 0$ has the same distributions as $\zeta,$ and $\bar{\zeta}$ for $-x\geq0$.

The construction of branching-like processes enables us to consider a generic time $k\leq nt$ and study the local times at a time $k$ from the Markov processes $\zeta$ and $\bar{\zeta}$. An advantage is that a typical event (\TBD{see equations to be defined in section \ref{sec: approximations}}) on a collection of spatial points will be also a typical event on a 'typical' single site. Then expressions like \eqref{eq: conditional mean in GPU represenetation} reduces the problem to a problem involving generalized P\'{o}lya urn process associated to a single site. The difficulty is then transferred to constructing typical events that can be estimated. For example, Lemma \ref{lm: number of rarely visit sites} below describes a typical event, and it reduces the proof of proposition \ref{lm: approximation of means of local drift} to \eqref{eq: convergence of conditional expectation} below. In the following subsection, we recall some properties of generalized P\'{o}lya urn process and branching-like processes.

\subsection{Preliminary Results}
To facilitate our arguments in section \ref{sec: approximations}, we list three \textcolor{red}{(or four)} results from \cite{KMP22,T96}, whose proofs \textcolor{red}{we omit at this moment.}

The first result is a concentration inequality for $\mathcal{D}_i$ in a generalized P\'{o}lya urn process. This lemma is a major tool in estimating the probabilities of good events.
\begin{lemma}(Lemma 4.1 \cite{KMP22})\label{lm: concentration inequality}
	Let weights $r(i) = w(2i)$, $b(i)= w(2i+1) $ for all $i\geq 0$. Then there exists constants $C,c>0$ such that for $k, m \in \mathbb{N}$,
	$$
	P\left(  \abs{ \mathcal{D}_{\tau_k^B}   } \geq m \right) \leq C e^{\frac{-cm^2}{m \vee k}}.
	$$
\end{lemma} 
Lemma \ref{lm: concentration inequality} remains valid for the generalized P\'{o}lya urn process $(\mathcal{B}_{k},\mathcal{R}_{k})_k$ associated to sites $y<0$ and $y=0$. For these two cases, the sequence of weights $(r(i),b(i))$ are slightly different, see \eqref{eq: generalized weights}. When $y<0$, $r(i) = w(2i+1)$, $b(i)= w(2i) $; when $y=0$, $r(i) = b(i)=w(2i)$.

The second result is an identity for a generalized P\'{o}lya urn process $(\mathcal{B}_{k},\mathcal{R}_{k})_k$. For any $k\geq 0$ denote by $\mu(k)= \tau^B_k - k$ the number of red balls extracted before the $k$-th blue ball. 
\begin{lemma}(Lemma 1, \cite{T96}) \label{lm: Toth's Identity}
	For any $m\in \mathbb{N}$ and $\lambda < \min\{ b(j): 0\leq j\leq m-1 \}$, we have the following identity,
	$$  \mathbb{E}\left[  \prod_{j=0}^{ \mu(m)-1 } \left(1+ \frac{\lambda}{r(j)}   \right) \right] =   \prod_{j=0}^{ m-1 } \left(1- \frac{\lambda}{b(j)}   \right)^{-1}.   $$ 
	In particular, 
	\begin{equation}\label{eq: Toth's Identity 1}
		\mathbb{E}\left[  \sum_{j=0}^{ \mu(m)-1 } \frac{1}{r(j)}   \right] =   \sum_{j=0}^{ m-1 } \frac{1}{b(j)}.
	\end{equation}	
\end{lemma}
\eqref{eq: Toth's Identity 1} is a direct consequence of the first identity. And the first identity can be proved via (exponential) martingales associated to the generalized P\'{o}lya urn process $(\mathcal{B}_{k},\mathcal{R}_{k})$, 
$$M_k(\lambda) = \prod_{i=0}^{ \mathcal{B}_{k}-1 } \left(1-\frac{\lambda}{b(i)}\right) \prod_{j=0}^{\mathcal{R}_{k}-1 } \left(1+\frac{\lambda}{r(j)}\right). $$

The third result is about the diffusion approximations of the branching-like processes. It is due to Toth \cite{T96}, and plays a role in the proof of the process level tightness of extrema. 

\begin{lemma}(Proposition A.3 \cite{KMP22})\label{lm: diffusion approximation of blp}
	For $n\geq 1$, let $\zeta^{(n)}=(\zeta^{(n)}_k)_{k\geq 0 }  $ be the BLP with initial value $\zeta^{(n)}_0 = \lfloor yn \rfloor$ for some $y \geq 0$, and let $\mathcal{Z}_n(t) = \frac{\zeta^{(n)}_{\lfloor nt \rfloor}}{n}$ for $n\geq 1$ and $t\geq 0$. Then we have that 
	$$
	\mathcal{Z}_n(.) \Longrightarrow Z^{(2-2\gamma)}(.)
	$$ as $n$ goes to infinity on $D([0,\infty)),$ where $Z^{(2-2\gamma)}(.)$ is the squared Bessel processes of dimension $2-2\gamma$.
\end{lemma}

The last result is a result from the branching-like processes. It gives a control of the number of sites with "small" local times:
\begin{lemma}(Lemma 2.2 \cite{KMP22})\label{lm: number of rarely visit sites}
	Let $\alpha =0$, and $\gamma_+ = \gamma \vee 0$. Then for any $M>0$, and any $b>\frac{\gamma_+}{2}$ we have
	$$
	\lim_{n\to\infty} P\left(\sup_{k\leq nt}  \sum_{x\in [I^X_{k-1}, S^X_{k-1}]} \mathbb{1}_{\{ L(x,k-1) \leq M \}} \geq 4n^b \right) = 0.
	$$
	
\end{lemma}	
Lemma \ref{lm: number of rarely visit sites} is a technical result, and its proof involves the analysis of BLPs and the concentration inequality in Lemma \ref{lm: concentration inequality} for the generalized P\'{o}lya urn process. The statement of Lemma \ref{lm: number of rarely visit sites} remains in force if we replace the range $[I_{k-1}, M_{k-1}]$ by $[X_k,M_k]$ (or $[I_{k-1},X_k]$ respectively), and replace the local times $L(x,k-1)$ by the numbers of up-crossings $\mathcal{E}^{k}_x$ (or $D^{k}_x$ respectively). In fact, these two extended results are partial steps in the proof of Lemma 2.2 in \cite{KMP22}.   


\section{Approximations of Local Drifts}\label{sec: approximations}
In this section, we will prove the technical propositions in the proof of Theorem \ref{thm: main}. \TBD


\subsection{Convergence of conditional Expectation}
In view of the generalized P\'{o}lya urn process (associated to a site $y> x$), on the event that $ L = \mathcal{E}^{(x,m)}_{y-1}$, we have \eqref{eq: conditional mean in GPU represenetation} 
$$\rho^{(x,m)}_y = E[\mathcal{D}_{\tau_L^B}].$$ 
On one hand, $M_{nt} \leq K\sqrt{n} $ for some $K>0$ with a high probability; on the other hand, Lemma \ref{lm: number of rarely visit sites} says that, up to $n^b$ sites, (where $\frac{\gamma \vee 0}{2}<b<\frac{1}{2}$,) every site $y$ between $X_k=x$ and $\mathcal{M}^{(x,m)} =S_{k}^X$ has $ \mathcal{E}^{(x,m)}_{y-1} \geq M  $ with a high probability. To show Proposition \ref{lm: approximation of means of local drift}, it suffices to show 
\begin{equation}\label{eq: convergence of conditional expectation}
	\lim_{M\to\infty} E[\mathcal{D}_{\tau_M^B}] = \gamma , 
\end{equation} for positive sites. A symmetric argument allows us to get the factor $sgn(y)$ for sites $y<0$ and $y=0$.

\begin{lemma} \label{lm: convergence of mean of discrepancies}
For the generalized P\'{o}lya urn process $(\mathcal{B}_{k},\mathcal{R}_{k})$ with weights $r(i)= w(2i)$, $b(i) = w(2i+1)$ for all $i\geq 0$, we have that
$$
\lim_{M\to\infty} E[\mathcal{D}_{\tau_M^B}] = \gamma. 
$$
\end{lemma} 
\begin{proof} 
	 We start from \eqref{eq: gamma} and identity \eqref{eq: Toth's Identity 1}. For any $m \geq 10$,
	 \begin{align}
	 	 V_1(m) - U_1(m) =& \sum_{i=0}^{m-1} \frac{1}{w(2i+1)} -\sum_{i=0}^{m-1} \frac{1}{w(2i)} 
	 	 \notag \\
	 	 =& \sum_{i=0}^{m-1} \frac{1}{b(i)} -\sum_{i=0}^{m-1} \frac{1}{r(i)} 
	 	 \notag \\
	 	 =& 	\mathbb{E}\left[  \sum_{j=0}^{ \mu(m)-1 } \frac{1}{r(j)}   \right] - \sum_{i=0}^{m-1} \frac{1}{r(i)} = \mathbb{E}\left[  \sum_{j=0}^{ \mu(m)-1 } \frac{1}{r(j)}    - \sum_{i=0}^{m-1} \frac{1}{r(i)}\right]. \label{eq: difference}
	 \end{align}
	From \eqref{eq: asymptotics of w}, we have that $0< \inf \frac{1}{r(j)} \leq \sup \frac{1}{r(j)} <\infty $, then $\mathbb{E}\left[\mu(m)\right]$ is bounded by
	$$\mathbb{E}\left[ \mu(m) \right] = \mathbb{E}\left[ \mu(m)\mathbb{1}_{\mu(m)\geq 1} \right] \leq  \frac{1}{\inf 1/w(j) }\mathbb{E}\left[  \sum_{j=0}^{ \mu(m)-1 } \frac{1}{r(j)}   \right] <\infty, $$ 
	so $ \mathbb{E}\left[ \mu(m) -m\right]  $ is bounded.
The difference in \eqref{eq: difference} can be written as
\begin{align} 
	\sum_{j=0}^{ \mu(m)-1 } \frac{1}{r(j)} - \sum_{i=0}^{m-1} \frac{1}{r(i)} =& \sum_{j=m}^{\mu(m)-1} \left(\frac{1}{r(j)} -\frac{1}{r(m)} \right) \cdot\mathbb{1}_{\{\mu(m)\geq m\}} 
		\label{eq: 1st term}
		\\	
	& - \sum_{j=\mu(m)}^{m-1} \left(\frac{1}{r(j)} -\frac{1}{r(m)} \right) \mathbb{1}_{\{\mu(m)< m\}} 
	\label{eq: 2nd term}
		\\
	& + \frac{\mu(m)-m}{ r(m) }. \label{eq: major term}
\end{align} 
Since $\mu(m)-m =  \mathcal{D}_{\tau^B_m}$, the last term \eqref{eq: major term} is exactly $\frac{1}{r(m)} \mathcal{D}_{\tau^B_m}$, which has an expectation $\frac{1}{r(m)} \mathbb{E}\left[\mathcal{D}_{\tau^B_m}\right].$ Both \eqref{eq: 1st term} and \eqref{eq: 2nd term} have finite expectations, which vanish as $m$ goes to infinity:

 Indeed, let $A> \frac{2}{c} \vee 1$, where $c$ is from Lemma \ref{lm: concentration inequality}. \eqref{eq: 1st term} is bounded by
\begin{align}
	\sum_{j=m}^{\infty} \left(\abs{\frac{1}{r(j)} -\frac{1}{r(m)} } \cdot \mathbb{1}_{\{\mu(m)\geq j \}}\right) & \leq  \sum_{0\leq j-m \leq A \sqrt{m}\log m } \left(\abs{\frac{1}{r(j)} -\frac{1}{r(m)} } \cdot \mathbb{1}_{\{\mu(m)\geq j \}}\right) 
	\notag
	\\
	& +  \sum_{j-m > A\sqrt{m}\log m } \left(\abs{\frac{1}{r(j)} -\frac{1}{r(m)} } \cdot \mathbb{1}_{\{\mu(m)\geq j \}}\right)
	\notag
	\\
	&\leq  \sum_{0\leq j-m \leq A\sqrt{m}\log m } \abs{\frac{1}{r(j)} -\frac{1}{r(m)} }
	\label{low difference}
	\\
	& + 2\left(\sup_j \frac{1}{w(j)}\right) \cdot \sum_{j\geq m + A\sqrt{m}\log m } \mathbb{1}_{\{ \mu(m) > j \}}
	\label{large difference}
\end{align}
In view of \eqref{eq: asymptotics of w}, there is a constant $C'>0$ such that for any $m>100 $ and any $j$ with $\abs{j-m}\leq A \sqrt m \log m $, 
$$ \abs{\frac{1}{r(j)} -\frac{1}{r(m)} } \leq C' A m^{-p-\frac{1}{2}} \log m, $$
which implies that \eqref{low difference} is bounded by
$$
C' A^2 m^{-p} (\log m)^2.
$$ On the other hand, Lemma \ref{lm: concentration inequality} implies that the expectation of \eqref{large difference} is bounded by
\begin{align*}
 & 2\left(\sup_j \frac{1}{w(j)}\right) \sum_{j-m \geq A \sqrt m \log m  } P( \mathcal{D}_{\tau^B_m} \geq j-m )  
 \notag 
 \\
 \leq& 2\left(\sup_j \frac{1}{w(j)}\right) \sum_{l \geq A \sqrt m \log m } C \exp\left( - \frac{c  \cdot l^2}{l \vee m}   \right)
 \notag\\
 \leq& C'' \left( \exp (- cA^2 \cdot \log m ) + \exp(-cA \cdot \log m) \right), 
\end{align*} for some $C''$ independent of $m$. Therefore, the expectation of \eqref{eq: 1st term} is bounded by
\begin{equation}\label{boound}
	C' A^2 m^{-p} \log m + C''  \left( m ^{-cA^2} +  m^{-cA} \right). 
\end{equation}
One can treat \eqref{eq: 2nd term} similarly. 

With our choice of $A >\frac{2}{c} \vee 1$, \eqref{eq: difference}, \eqref{eq: 1st term}, \eqref{eq: 2nd term}, \eqref{eq: major term}, and \eqref{boound}, we get that
$$ \abs{ V_1(m)- U_1(m) -\frac{1}{r(m)}\mathbb{E}\left[ \mathcal{D}_{\tau^B_m} \right] }
\leq 2C' A^2 m^{-p} \log m + 2C''  \left( m ^{-cA^2} +  m^{-cA} \right), 
$$ which converges to $0$ as $m$ goes to infinity. We conclude that
$$
\lim_{m\to\infty}\mathbb{E}\left[ \mathcal{D}_{\tau^B_m} \right] = \gamma, 
$$ from $\lim_{m\to\infty}\frac{1}{r(m)} =1$ and $ \lim_{m\to \infty} \left(V_1(m)-U_1(m) \right) = \gamma$.
\end{proof}

For the generalized P\'{o}lya urn process associated to a site $y<0$, we have $r(i) = w(2i+1)$, $b(i) =w(2i)$. The right hand side of \eqref{eq: difference} is the same as $U_1(m)-V_1(m)$, which converges to $-\gamma$ as $m$ goes to infinity. Then we get that  $$\lim_{m\to\infty}\mathbb{E}\left[ \mathcal{D}_{\tau^B_m} \right] = -\gamma.$$
Similarly, for $y=0$, the associated generalized P\'{o}lya urn process has 
$$\lim_{m\to\infty}\mathbb{E}\left[ \mathcal{D}_{\tau^B_m} \right] = 0.$$

\TBD \textcolor{red}{We will need to define $\rho$ for negative sites.} We extend the definition of $\rho^{(x,m)}_y$ for sites $y< x$ by symmetry, 
$$\rho^{(x,m)}_y := \mathbb{E}\left[  \Delta^{(x,m)}_y   \left\vert  \sigma\left( D^{(x,m)}_z:  y<z\leq  x  \right. \right) \right].   $$ In terms of the generalized P\'{o}lya urn process associated to the site $y$,
\begin{equation} \label{eq: extended definition}
	\rho^{(x,m)}_y = \mathbb{E}\left[\mathcal{D}_{\tau_L^R}\right], 
\end{equation} where $L = D^{(x,m)}_{y+1} = \mathcal{E}^{(x,m)}_{y}.$ With an argument similar to the proof of Lemma \ref{lm: convergence of mean of discrepancies}, we get that 
\begin{equation}\label{eq: mean of discrepancies for left sites}
\lim_{L\to \infty} \mathbb{E}\left[\mathcal{D}_{\tau_L^R}\right] =  sgn(y) \cdot \gamma = \lim_{L\to \infty} \mathbb{E}\left[\mathcal{D}_{\tau_L^B}\right].
\end{equation}

Now we show Proposition \ref{lm: approximation of means of local drift} and a slightly stronger result. 
\begin{lemma}
	Let $w(.)$ be a positive monotone function on $\mathbb{N}_0$ satisfying \eqref{eq: asymptotics of w}. Then for any $0<p<1$, any $\epsilon>0$,
	$$
	\lim_{n\to\infty} P\left( \sup_{k\leq n t}  \abs{  	\sum_{y\in [X_{k}+1 ,S_{k}^X]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \geq  \epsilon \sqrt{n}     \right) =0.
	$$
	Furthermore,
		$$
	\lim_{n\to\infty} P\left( \sup_{k\leq n t}  \abs{  	\sum_{y\in [I_k^{X} ,S_{k}^X]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \geq  \epsilon \sqrt{n}     \right) =0.
	$$
\end{lemma}
\begin{proof} There are only three types of weight sequences for the generalized P\'{o}lya urn processes, see \eqref{eq: generalized weights}. Therefore, from Lemma \ref{lm: convergence of mean of discrepancies}, and \eqref{eq: mean of discrepancies for left sites}, there is a decreasing function $C(.)$ on $\mathbb{N}_0$ with $\lim_{L\to \infty}C(L) =0$ such that for any $y \in \mathbb{Z}$,
	\begin{equation}\label{eq: uniform convergence}
		\abs{\mathbb{E}\left[ \mathcal{D}_{\tau_L^R} \right] - \gamma \cdot sgn(y)}, \abs{\mathbb{E}\left[ \mathcal{D}_{\tau_L^B} \right] - \gamma \cdot sgn(y)} \leq C(L).
	\end{equation} One such function is $C(l) = \sup \left\{  \abs{\mathbb{E}\left[ \mathcal{D}_{\tau_m^R} \right] - \gamma \cdot sgn(y)} + \abs{\mathbb{E}\left[ \mathcal{D}_{\tau_m^B} \right] - \gamma \cdot sgn(y)} : m\geq l \right\}.     $  
	
	
	Let $t>0$, and $b \in [\frac{\gamma \vee 0 }{2},\frac{1}{2})$.  For any $n,K,M>0$, we consider three types of events, 
	$$A_{n,K}:=\left\{ \min\{-I_{nt}, M_{nt}\} \geq K \sqrt{n}  \right\}$$
	$$B_{n,M}:= \left\{  \sup_{k\leq n t} \sum_{ y\in (X_{k-1}, M_{k-1}]}  \mathbb{1}_{\{ \mathcal{E}^{k-1}_{y-1} \leq M  \}} >n^b  \right\},  $$
	and 
	$$B'_{n,M}:=  \left\{  \sup_{k\leq n t} \sum_{ y\in [I_{k-1}, X_{k-1})}  \mathbb{1}_{\{ \mathcal{D}^{k-1}_{y+1} \leq M  \}} >n^b  \right\}.$$
Clearly, $A_{n,K}$ is decreasing in $K$, and $B_{n,M}, B'_{n,M}$ are increasing in $M$. We claim that for $n$ large, the event 
$$
F_{n,\epsilon}:= \left\{ \sup_{k\leq n t}  \abs{  	\sum_{y\in [X_{k}+1 ,M_k]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \geq  \epsilon \sqrt{n}    \right \}$$ is contained in $A_{n,K} \cup B_{n,M} $ for some finite $K, M$ independent of $n$:   

	Indeed, depending on $(\mathcal{E}^{k-1}_{y-1} \leq M)$, terms  $\left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right)$ are bounded by $C(0)$ or $C(M)$. Therefore, the supremum is bounded by
	\begin{align*}
	 \sup_{k\leq n t}  \abs{  	\sum_{y\in [X_{k}+1 ,M_k]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \leq &  
	 C(0) \cdot \sup_{k\leq n t} \left\{   	\sum_{y\in [X_{k}+1 ,M_k]} \mathbb{1}_{ \{ \mathcal{E}^{k-1}_{y-1} \leq M \} } \right\}
	 \notag
	 \\
	 +& C(M) \cdot \sup_{k\leq n t} \left\{   	\sum_{y\in [X_{k}+1 ,M_k]} \mathbb{1}_{ \{ \mathcal{E}^{k-1}_{y-1} \geq M \} } \right\},
\end{align*} which is bounded on $A^c_{n,K} \cap B^c_{n,M}$ by
\begin{equation}\label{eq: an upper bound on good set}
	C(0)n^b  + C(M) \left(K \sqrt{n} -n^b\right).
\end{equation} As $n$ goes to infinity, \eqref{eq: an upper bound on good set} is smaller than $\epsilon \sqrt{n}$ for any $K>0$ and any $M$ with $C(M) < \frac{\epsilon}{2K}$ . 
For such pairs of $(K,M)$, $A^c_{n,K} \cap B^c_{n,M} \subset F^c_{n,\epsilon}$ when $n$ is large,  and 
$$
\limsup_{n\to \infty} P(F_{n,\epsilon}) \leq \limsup_{n\to \infty}  P(A_{n,K}) +  \limsup_{n\to \infty}  P(B_{n,M}).
$$ In view of Lemma \ref{lm: number of rarely visit sites} and the explanation after it, the second term $$\limsup_{n\to \infty}  P(B_{n,M})=0.$$  The first term $\limsup_{n\to \infty}  P(A_{n,K}) $ vanishes as $K$ goes to infinity, which is a consequence of Proposition 2.1 \cite{KMP22}, or Corollary 1A \cite{T96}.
 
The proof of 
$$  
\lim_{n\to\infty} P\left( \sup_{k\leq n t}  \abs{  	\sum_{y\in [I_k ,M_k]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \geq  \epsilon \sqrt{n}     \right) =0
$$ 
follows a similar argument. In particular, if we replace the range from $[X_k+1, M_k]$ by $[I_k,M_k]$, the event 
$$
\tilde{F}_{n,\epsilon}:= \left\{ \sup_{k\leq n t}  \abs{  	\sum_{y\in [X_{k}+1 ,M_{k}]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right) } \geq  \epsilon \sqrt{n}    \right \}
$$ 
is contained in $A_{n,K} \cup B_{n,M} \cup  B'_{n,M},$ for any $K>0$, and $M$ with $C(M) < \frac{\epsilon}{4K}$.
\end{proof}

\eqref{eq: an upper bound on good set} can be used as a crude estimate for 	$\sum_{y\in [X_{k}+1 ,M_{k}]} \left( \rho^{(X_k,L(X_k,k))}_y -  \gamma \cdot sgn(y) \right)$   on the "good events" $A^c_{n,K}\cap B^c_{n,M}$.

\subsection{Approximation of Local Drifts by Conditional Means}
In this subsection, we prove Proposition \ref{lm: approx local drift by conditional means}. \textcolor{red}{\TBD. Its proof is composed of by an argument similar to that on page 20 of \cite{KP16}.}

By symmetry we only present the proof for $x >  0$. \TBD. We are currently only considering sites right to $x$, i.e. $y \ge x$. Let $m > 0$.
We work on the event $\left\{ X_n = x, n = \lambda_{x, m} \right\} $. Consider some $K>0$ and define the good event

\begin{align}
	G_K^{(x,m)} :=  \qquad
		\label{eqn:good-event-1}
		& \left\{ M^{(x,m)}< K \sqrt{n}\right\} \cup  \\ 
		\label{eqn:good-event-2}
		& \bigcap_{y \ge x} \left\{\mathcal{E}_y^{(x,m)} < K \sqrt{ n} \right\}  \cup \\
		\label{eqn:good-event-3}
		& \bigcap_{y \ge x} \bigcap_{i < K \sqrt{n} } \left\{\left| \tau_i^{\mathcal{B}} - 2 i \right| < \sqrt{ i } \log^2 n \right\}  \cup \\
		\label{eqn:good-event-4}
		& \bigcap_{y \ge x} \bigcap_{i < K \sqrt{n} } \left\{\left| \tau_{i+1}^{\mathcal{B}} - \tau_i^{\mathcal{B}} \right| < \log^2 n \right\}  \\
		&=: A_K^{(x,m)} \cup B_{K}^{(x,m)} 
		\cup C_{K}^{(x,m)}
		\cup D_{K}^{(x,m)} 
.\end{align}

From process-level tightness we know that probability of \eqref{eqn:good-event-1} goes to $1$ as $K \to \infty $. By taking $K$ large we can also establish the likelihood of \eqref{eqn:good-event-2}, easily verifiable by the following lemma:

\begin{lemma}
	For arbitrary $x, m, n>0$, we have
	\[
		\lim_{K \to \infty} 
		P\left( \bigcap_{y \ge x} \left\{\mathcal{E}_y^{(x,m)} < K \sqrt{ n} \right\} \right) = 1
	.\] 
\end{lemma}
\begin{proof}
	We prove the probability of the complement goes to $0$. If there exists some $y \ge x$ such that $\mathcal{E}_y^{(x,m)} \ge  K \sqrt{n} $, then there must exist $z>x$ such that $\mathcal{E}_{z+1}^{(x,m)} \ge \mathcal{E}_{z}^{(x,m)} + K \sqrt{n} / y $.

	From the process-level tightness, for any $\varepsilon>0$ there exists $M_\varepsilon$ such that $P\left( M^{(x,m)} - I^{(x,m)} < \sqrt{n}  \right) \ge 1 - \varepsilon$. On this event, we would have $\mathcal{E}_{z+1}^{(x,m)} \ge \mathcal{E}_{z}^{(x,m)} + K$. The probability that this happens goes to $0$ as we make $K$ large.
\end{proof}

To establish the likelihood of \eqref{eqn:good-event-3} and \eqref{eqn:good-event-4}, note that the inner intersection gives events i.i.d. over $y$, so it suffices to control the complement of one of them.

\begin{align*}
	P\left(\left( C_{K}^{(x,m)}\right)^c |A_{K}^{(x,m)}, B_K^{(x,m)}\right) 
	&\le \sum_{y < \sqrt{n} }\sum_{i < K \sqrt{ n} } P\left( |\tau_i^{\mathcal{B}} - 2i| \ge \sqrt{i} \log^2 n \right) \\
	&\le CK \sqrt{n} \sum_{i < K \sqrt{ n} } \exp\left( - c \frac{i \log^4 n}{\sqrt{i}  \log^2 n \vee i} \right)  \\
	&\le CK \sqrt{n}  \sum_{i < K \sqrt{ n} }  
	\left( \exp\left( - c \sqrt{i}  \log^2 n \right)  + 
	\exp\left( - c \log^4 n \right) \right)
.\end{align*}
Where in the third line we used Lemma~\ref{lm: concentration inequality}. For fixed $K$, the last line goes to $0$ as $n \to \infty $. For \eqref{eqn:good-event-4}, we make a similar argument and note that, since the probability that the next ball drawn is blue is bounded below by some constant $q > 0$, $\tau_{i+1}^{\mathcal{B}} - \tau_{i}^{\mathcal{B}}$ is stochastically dominated by some geometric random variable $\text{Geo}(q)$. Hence

\[
	P\left(\left( D_{K}^{(x,m)}\right)^c |A_{K}^{(x,m)}, B_K^{(x,m)}\right) 
	\le C K^2 n \exp\left( - c \log^2 n \right) 
.\] 

From the arguments above we have concluded that $P\left( G_K^{(x,m)} \right)  \to 1$ for all $x$, all large $K$ and all large $m$ depending on $K$.

On the good event $G_{K}^{(x,m)}$ we can have a deterministic bound for $|\Delta_y^{(x,m)}|$, which puts admissible conditions to the martingale in \eqref{eq: control of martingale difference for local drift}.

\begin{lemma}\label{lm:lipchitz-bound-on-good-event}
	When $p \in (0,\frac{1}{2}]$, on $G_{K}^{(x,m)}$, we have for all  $y \ge x$,
	\[
		\left| \Delta_y^{(x,m)} \right| \le C_K n^{-\frac{1}{2}p + \frac{1}{4}} \log^4 n
	.\] 
	When $p = \frac{1}{2}$, we have the bound
\[
		\left| \Delta_y^{(x,m)} \right| \le C_K \log^5 n
	.\]
	As a corollary, those bounds also apply to $\left| \Delta_y^{(x,m)} - \rho_y^{(x,m)} \right| $, up to multiplicative constant.
\end{lemma}

Having this lemma, we can apply Azuma's inequality to the sum in \eqref{eq: control of martingale difference for local drift}. In the case $p < \frac{1}{2}$ we get for all $\varepsilon>0$,

\begin{align}
	P\left( \left| \sum_{y\ge X_n} (\Delta_y^{(x,m)} - \rho_y^{(x,m)})  \right| > \varepsilon \sqrt{n}  \right) \notag
	&\le \exp\left( - \frac{\varepsilon^2 n}{2 K \sqrt{n} \left( C_{K} n^{-\frac{1}{2}p + \frac{1}{4}} \log^2 n  \right)^2 } \right) + P(G^c) \notag \\
	&= \exp\left( - C_{K, \varepsilon} \, n^{p } \log^{-4} n \right) +P(G^c) \label{eqn:azuma-drift-martingale}
.\end{align}
In the case $p = \frac{1}{2}$ the bound becomes
\begin{align}
	P\left( \left| \sum_{y \ge X_n} (\Delta_y^{(x,m)} - \rho_y^{(x,m)}) \right|  > \varepsilon \sqrt{n}  \right) \notag
	&\le  \exp\left( - C_{K, \varepsilon} \, \sqrt{n}  \log^{-5} n \right) +P(G^c) 
.\end{align}
In both cases, we have the right hand side go to $0$ as $n \to  \infty $. As a final step, we need to strengthen the statement to a probabilistic bound for $\sup_{k < nt} \left| \sum_{y \ge X_k} (\Delta_y^{(x,m)} - \rho_y^{(x,m)}) \right|$. \TBD: Apply a union bound over all possible $(x,m)$ pairs. This will give the wanted result since both the exponential term and $P((G_K^{(x,m)})^c)$ still goes to zero after multiplication by $n$.

\begin{proof}[Proof of Lemma~\ref{lm:lipchitz-bound-on-good-event}]
	\begin{equation*}
	\left| \Delta_y^{(x,m)} \right| 
	= 
	\left| 	\sum_{i = 0}^{\mathcal{E}_y^{(x,m)}} 
	\sum_{l = \tau_i^{\mathcal{B}}} ^{\tau_{i+1}^{\mathcal{B}}  -1}
	\alpha(\mathcal{B}_l, \mathcal{R}_l)
	\right| 
	\le 
	\sum_{i = 0}^{K \sqrt{n} } 
	\sum_{l = \tau_i^{\mathcal{B}}} ^{\tau_{i+1}^{\mathcal{B}}  -1}
	\left|
	\alpha(\mathcal{B}_l, \mathcal{R}_l)
	\right| 
	.
\end{equation*}

\begin{align*}
	&\sum_{i = 0}^{K \sqrt{n} } 
	\sum_{l = \tau_i^{\mathcal{B}}} ^{\tau_{i+1}^{\mathcal{B}}  -1}
	|\alpha(\mathcal{B}_l, \mathcal{R}_l)|
	:=
	\sum_{i = 0}^{K \sqrt{n} } 
	\sum_{l = \tau_i^{\mathcal{B}}} ^{\tau_{i+1}^{\mathcal{B}}  -1}
	\left|\frac{-w(2 \mathcal{B}_l + 1) + w(2 \mathcal{R}_l)}{w(2\mathcal{B}_l + 1) + w(2 \mathcal{R}_l)}\right|\\
	&\le C_w \sum_i \sum_l \left| \frac{1}{w(2 \mathcal{R}_l)} - \frac{1}{w(2 \mathcal{B}_l + 1)} \right|  \\
	&= C_w \sum_i \sum_l \left| \frac{1}{w(2 l - 2 i)} - \frac{1}{w(2i + 1)} \right|  \\
	&= C_w \sum_i \sum_{j = \tau_i^{\mathcal{B}} - i}^{\tau_{i+1}^{\mathcal{B}} - i - 1} \left| \frac{1}{w(2j)} - \frac{1}{w(2i + 1)} \right|  \\
	&= C_w \sum_i \sum_{j = \tau_i^{\mathcal{B}} - i}^{\tau_{i+1}^{\mathcal{B}} - i - 1} \left|  2^p B\left( \frac{1}{(2j)^p} - \frac{1}{(2i + 1)^p} \right)  + O\left( \frac{1}{i^{\kappa + 1}} \right) \right|  \\
	&\le C_w \sum_i \log^2 n \sup_{|j - i| \le 2 \sqrt{i}  \log^2 n} \left|  2^p B\left( \frac{1}{(2j)^p} - \frac{1}{(2i + 1)^p} \right)  + O\left( \frac{1}{i^{\kappa + 1}} \right) \right| \\
	&\le C_{w, p} \sum_i \log^2 n \left( 
		(4 \sqrt{ i } \log^2 n) (2 i - 2 \sqrt{ i } \log^2 n)^{- p - 1} + (2 i)^{- p - 1} + O(i^{- \kappa - 1})
	\right)  \\
	&\le C_{w, p} \sum_i \log^2 n \left( i ^{-p - \frac{1}{2}} \log^2 n +  i^{- \kappa - 1} \right)  \\
	&= C_{w, p} \sum_i i^{- p - \frac{1}{2}} \log^4 n + i^{- \kappa - 1 } \log^2 n \\
	&\stackrel{(p<\frac{1}{2})}{\le } C_{w, p} \left( (K \sqrt{ n} )^{-p+ \frac{1}{2}} \log^4 n + (K \sqrt{ n} )^{- \kappa } \log^2 n \right)  \\
	&= C_{w, p, K} n^{-\frac{1}{2}p + \frac{1}{4}  }  \log^4 n \qquad \text{assuming $\kappa$ small}
.\end{align*} 

\end{proof}



\subsection{Control of martingale Terms } 
In this subsection, we prove Proposition \ref{lm: control of martingale}. It is already remarked earlier that we only need to obtain the control

\[
	 \lim_{n \to \infty } \frac{1}{n} \sum_{i = 0}^{n-1} \mathbb{E}\left[ X_{i+1} - X_i | \mathcal{F}_i \right]^2 = 0
.\] 

\textcolor{red}{\TBD. Its proof is a similar to the proof of Proposition \ref{lm: approx local drift by conditional means}}.

We can rewrite the sum into a sum of local drifts, and isolate the first $M$ visits:

\begin{align}
	&\sum_{i = 0}^{n-1} \mathbb{E}\left[ X_{i+1} - X_i | \mathcal{F}_i \right]^2\\
	&= \sum_{x \in \left[ I_n^X, S_n^X \right]} \sum_{i = 0}^{L_x(n) - 1} \mathbb{E}\left[ \mathcal{D}_{i+1}^{(x)} - \mathcal{D}_i^{(x)} | \mathcal{F}_{i}^{\mathcal{B}, \mathcal{R}} \right]^2  \\
	&\le  \sum_{x \in \left[ I_n^X, S_n^X \right]} \sum_{i = 0}^{L_x(n) - 1} \mathbb{E}\left[ \mathcal{D}_{i+1}^{(x)} - \mathcal{D}_i^{(x)} | \mathcal{F}_{i}^{\mathcal{B}, \mathcal{R}} \right]^2 \mathbf{1}\left( L_x(i) > M \right) + 
	\sum_{i = 0}^{n - 1} \mathbf{1}\left( L_{X_i}(i) \le  M \right) 
	\label{eqn:lem-martingale-1}
.\end{align}

We now restrict ourselves to the good event $G$ stated in \eqref{eqn:good-event-1}-\eqref{eqn:good-event-2}. Under $G$,
the inner sum in the first term is further controlled by
\begin{align*}
	&\sum_{i =0}^{ L_x(n) - 1} \mathbb{E}\left[ \mathcal{D}_{i+1}^{(x)} - \mathcal{D}_i^{(x)} | \mathcal{F}_{i}^{\mathcal{B}, \mathcal{R}} \right]^2 \mathbf{1}\left( L_{x}(i) > M \right) \\
	&\stackrel{(x > 0)}{\le} C_w \sum_{i = M}^{\mathcal{B}_n} \sum_{l = \tau_i^{\mathcal{B}}}^{\tau_{i+1}^{\mathcal{B}}-1} 
	\left| \frac{1}{w(2 \mathcal{R}_l)} - \frac{1}{w\left( 2 \mathcal{B}_l + 1 \right) } \right|^2 \\
	&\le C_{w, p} \sum_i i^{- 2 p - 1} \log^3 n \\
	&\le C_{w, p} \left[ M^{- 2 p} - \mathcal{B}_n^{- 2 p} \right] \log^3 n  \\
	&< C_{w, p} M^{-2 p} \log^3 n
	.
\end{align*}

Note that this bound is uniform in $x$. We remark that cases $(x=0)$ and $(x < 0)$ have the exactly same bounds. 

Now apply the bound $\sum_{i = 0}^{n-1} \mathbf{1}\left( L_{X_i}(i) \le M \right) \mathbf{1}(X_i = x) \le  M$ to the second term in \eqref{eqn:lem-martingale-1}, we have

\begin{multline}
	\sum_{x \in \left[ I_n^X, S_n^X \right]} \sum_{i = 0}^{L_x(n) - 1} \mathbb{E}\left[ \mathcal{D}_{i+1}^{(x)} - \mathcal{D}_i^{(x)} | \mathcal{F}_{i}^{\mathcal{B}, \mathcal{R}} \right]^2 \\
	< \sum_{x \in \left[ I_n^X, S_n^X \right]} (C_{w, p} M^{-2p} \log^3 n +M )
\end{multline}

Using process-level tightness, the summation only add a term of order $\sqrt{n}$, on a good event. Since we have a factor of $\frac{1}{n}$, and $M$ independent of $n$, we have the right hand side $\to 0$. More precisely,

\begin{align*}
	 &P\left( \left| \frac{1}{n} \sum_{i = 0}^{n-1} \mathbb{E}\left[ X_{i+1} - X_i | \mathcal{F}_i \right]^2  \right|  > \varepsilon \right)\\
	 &\le P\left( \left| \sum_{x \in \left[ I_n^X, S_n^X \right]} (C_{w, p} M^{-2p} \log^3 n +M ) \right| > \varepsilon  n \right) + P(G^c) \\
	 &\le 2 P\left( S_n^X \ge n^{3 / 4} \right) + P\left(  \left| \sum_{|x| \le n^{3 / 4}} (C_{w, p} M^{-2p} \log^3 n +M ) \right| > \varepsilon  n  \right) +P(G^c)  \\
	 &\le 2 P\left( S_n^X \ge n^{3 / 4} \right) + P\left(  C_{w, p, M} \,\, n^{3 / 4} \log^3 n > \varepsilon  n  \right) + P(G^c)
.\end{align*}
The second term vanishes trivially; the first term vanishes by process-level tightness of $S_n^X / \sqrt{n} $. The last term goes to zero as shown in the previous section.


\begin{thebibliography}{99}\addcontentsline{toc}{chapter}{Bibliography} 
 \bibitem[KMP22]{KMP22} Kosygina,~E., Mountford,~T., Peterson,~J.: Convergence and Non-Convergence of Scaled Self-Interacting Random Walks to Brownian Motion Perturbed at Extrema. \textit{arXiv:2208.02589,} 2022.
 
 \bibitem[KP16]{KP16} Kosygina,~E., Peterson,~J.: Functional limit laws for recurrent excited random walks with
 periodic cookie stacks. \textit{Electron. J. Probab.} \textbf{21} 2016.

\bibitem[L23]{L23} Liu,~X.: Research in Random Walk. \textit{Personal communication,} 2023.

\bibitem[T96]{T96} T\'{o}th,~B.: Generalized Ray-Knight theory and limit theorems for self-interacting random walks on $\mathbb{Z}^1$ \textit{Ann. Probab.} \textbf{24} 1324--1367. 1996
\end{thebibliography}
\end{document}
